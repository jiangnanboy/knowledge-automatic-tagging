{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![textcnn模型](img/textcnn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>“重农抑商”政策</th>\n",
       "      <th>不完全显性</th>\n",
       "      <th>与细胞分裂有关的细胞器</th>\n",
       "      <th>中央官制——三公九卿制</th>\n",
       "      <th>中心体的结构和功能</th>\n",
       "      <th>人体免疫系统在维持稳态中的作用</th>\n",
       "      <th>人体水盐平衡调节</th>\n",
       "      <th>人体的体温调节</th>\n",
       "      <th>人口增长与人口问题</th>\n",
       "      <th>...</th>\n",
       "      <th>胚胎移植</th>\n",
       "      <th>蛋白质的合成</th>\n",
       "      <th>血糖平衡的调节</th>\n",
       "      <th>走进细胞</th>\n",
       "      <th>选官、用官制度的变化</th>\n",
       "      <th>遗传的分子基础</th>\n",
       "      <th>遗传的细胞基础</th>\n",
       "      <th>避孕的原理和方法</th>\n",
       "      <th>郡县制</th>\n",
       "      <th>高尔基体的结构和功能</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>左传 记载 春秋 后期 鲁国 大夫 季孙氏 家臣 阳虎 独掌 权柄 后 标榜 鲁国 国君 整...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>秦始皇 统一 六国后 创制 一套 御玺 任命 国家 官员 封印 皇帝 之玺 任命 四夷 官员...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>北宋 中央集权 措施 将领 兵权 收归 中央 派 文官 担任 地方 长官 设置 通判 监督 ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>商朝人 崇信 鬼神 占卜 祭祀 神灵 沟通 手段 负责 通神 事务 商王 巫师 出身 贵族 ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>公元 年 北宋 政府 江淮地区 设置 包括 盐业 管理 控制 茶叶 销售 专卖 主要职责 转...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29808</th>\n",
       "      <td>纯种 高杆 抗 锈病 小麦 矮杆 易染 锈病 小麦 培育 矮杆 抗 锈病 小麦 新品种 方法...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29809</th>\n",
       "      <td>下图 二倍体 生物 细胞分裂 受精 作用 过程 中核 含量 染色体 数目 变化 正确 孟德尔...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29810</th>\n",
       "      <td>调查 人群 中 遗传病 叙述 错误 选取 群体 中 发病率 高 基因 遗传病 患者 家庭成员...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29811</th>\n",
       "      <td>下图 人类 一种 遗传病 家系 图谱 图中 阴影 患者 推测 病 遗传 方式 常 染色体 显...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29812</th>\n",
       "      <td>下图 人类 某种 单 基因 遗传病 系谱 图为 患者 相关 叙述 该病 常 染色体 隐性 遗...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29813 rows × 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 content  “重农抑商”政策  不完全显性  \\\n",
       "0      左传 记载 春秋 后期 鲁国 大夫 季孙氏 家臣 阳虎 独掌 权柄 后 标榜 鲁国 国君 整...         1      0   \n",
       "1      秦始皇 统一 六国后 创制 一套 御玺 任命 国家 官员 封印 皇帝 之玺 任命 四夷 官员...         1      0   \n",
       "2      北宋 中央集权 措施 将领 兵权 收归 中央 派 文官 担任 地方 长官 设置 通判 监督 ...         1      0   \n",
       "3      商朝人 崇信 鬼神 占卜 祭祀 神灵 沟通 手段 负责 通神 事务 商王 巫师 出身 贵族 ...         1      0   \n",
       "4      公元 年 北宋 政府 江淮地区 设置 包括 盐业 管理 控制 茶叶 销售 专卖 主要职责 转...         1      0   \n",
       "...                                                  ...       ...    ...   \n",
       "29808  纯种 高杆 抗 锈病 小麦 矮杆 易染 锈病 小麦 培育 矮杆 抗 锈病 小麦 新品种 方法...         0      0   \n",
       "29809  下图 二倍体 生物 细胞分裂 受精 作用 过程 中核 含量 染色体 数目 变化 正确 孟德尔...         0      0   \n",
       "29810  调查 人群 中 遗传病 叙述 错误 选取 群体 中 发病率 高 基因 遗传病 患者 家庭成员...         0      1   \n",
       "29811  下图 人类 一种 遗传病 家系 图谱 图中 阴影 患者 推测 病 遗传 方式 常 染色体 显...         0      1   \n",
       "29812  下图 人类 某种 单 基因 遗传病 系谱 图为 患者 相关 叙述 该病 常 染色体 隐性 遗...         0      1   \n",
       "\n",
       "       与细胞分裂有关的细胞器  中央官制——三公九卿制  中心体的结构和功能  人体免疫系统在维持稳态中的作用  人体水盐平衡调节  \\\n",
       "0                0            1          0                0         0   \n",
       "1                0            1          0                0         0   \n",
       "2                0            1          0                0         0   \n",
       "3                0            1          0                0         0   \n",
       "4                0            1          0                0         0   \n",
       "...            ...          ...        ...              ...       ...   \n",
       "29808            0            0          0                0         0   \n",
       "29809            0            0          0                0         0   \n",
       "29810            0            0          0                0         0   \n",
       "29811            0            0          0                0         0   \n",
       "29812            0            0          0                0         0   \n",
       "\n",
       "       人体的体温调节  人口增长与人口问题  ...  胚胎移植  蛋白质的合成  血糖平衡的调节  走进细胞  选官、用官制度的变化  \\\n",
       "0            0          0  ...     0       0        0     0           0   \n",
       "1            0          0  ...     0       0        0     0           0   \n",
       "2            0          0  ...     0       0        0     0           1   \n",
       "3            0          0  ...     0       0        0     0           0   \n",
       "4            0          0  ...     0       0        0     0           1   \n",
       "...        ...        ...  ...   ...     ...      ...   ...         ...   \n",
       "29808        0          0  ...     0       0        0     0           0   \n",
       "29809        0          0  ...     0       0        0     0           0   \n",
       "29810        0          0  ...     0       0        0     0           0   \n",
       "29811        0          0  ...     0       0        0     0           0   \n",
       "29812        0          0  ...     0       0        0     0           0   \n",
       "\n",
       "       遗传的分子基础  遗传的细胞基础  避孕的原理和方法  郡县制  高尔基体的结构和功能  \n",
       "0            0        0         0    1           0  \n",
       "1            0        0         0    1           0  \n",
       "2            0        0         0    1           0  \n",
       "3            0        0         0    1           0  \n",
       "4            0        0         0    1           0  \n",
       "...        ...      ...       ...  ...         ...  \n",
       "29808        0        0         0    0           0  \n",
       "29809        1        0         1    0           0  \n",
       "29810        0        0         1    0           0  \n",
       "29811        0        0         1    0           0  \n",
       "29812        0        0         1    0           0  \n",
       "\n",
       "[29813 rows x 74 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# 加载知识点标签\n",
    "knowledge_points_path = os.path.join(os.getcwd(), \"knowledge_points.pkl\")\n",
    "with open(knowledge_points_path, 'rb') as f_words:\n",
    "    knowledge_points = pickle.load(f_words)\n",
    "    \n",
    "# 原始数据的处理见项目下的transformer-encoder模型中的train.ipynb\n",
    "# 这里直接加载处理好的训练数据\n",
    "train_data_path = os.path.join(os.getcwd(), \"train_data.pkl\")\n",
    "with open(train_data_path, 'rb') as f_train:\n",
    "    df_final = pickle.load(f_train)\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\torch\\storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchtext import data,datasets\n",
    "from torchtext.data import Iterator, BucketIterator\n",
    "from torchtext.vocab import Vectors\n",
    "from torch import nn,optim\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 按字分    \n",
    "tokenize =lambda x: x.split(' ')\n",
    "\n",
    "TEXT = data.Field(\n",
    "                    sequential=True,\n",
    "                    tokenize=tokenize,\n",
    "                    lower=False,\n",
    "                    use_vocab=True,\n",
    "                    pad_token='<pad>',\n",
    "                    unk_token='<unk>',\n",
    "                    batch_first=True,\n",
    "                    fix_length=200)\n",
    "\n",
    "LABEL = data.Field(\n",
    "                    sequential=False,\n",
    "                    use_vocab=False,\n",
    "                    batch_first=True,\n",
    "                    )\n",
    "\n",
    "# 获取训练或测试数据集\n",
    "def get_dataset(csv_data, text_field, label_field, test=False):\n",
    "    fields = [('id', None), ('text', text_field), ('label', label_field)]\n",
    "    examples = []\n",
    "    if test: #测试集，不加载label\n",
    "        for text in csv_data['content']:\n",
    "            examples.append(data.Example.fromlist([None, text, None], fields))\n",
    "    else: # 训练集\n",
    "        for i in range(len(csv_data)):\n",
    "            sample = csv_data.loc[i]\n",
    "            text = sample['content']\n",
    "            label = [v for v in map(int, sample[knowledge_points])]\n",
    "            examples.append(data.Example.fromlist([None, text, label], fields))\n",
    "    return examples, fields\n",
    "\n",
    "train_examples,train_fields = get_dataset(df_final, TEXT, LABEL)\n",
    "\n",
    "train = data.Dataset(train_examples, train_fields)\n",
    "# 预训练数据\n",
    "pretrained_embedding = os.path.join(os.getcwd(), 'sgns.sogou.char')\n",
    "vectors = Vectors(name=pretrained_embedding)\n",
    "# 构建词典\n",
    "TEXT.build_vocab(train, min_freq=1, vectors = vectors)\n",
    "\n",
    "#TEXT.build_vocab(train, min_freq=1)\n",
    "words_path = os.path.join(os.getcwd(), 'words.pkl')\n",
    "with open(words_path, 'wb') as f_words:\n",
    "    pickle.dump(TEXT.vocab, f_words)\n",
    "    \n",
    "print('process done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([70057, 300])\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build dataset done!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# 划分训练与验证集，一个问题，利用random_split进行数据集划分后，会丢失fields属性\n",
    "train_set, val_set = train.split(split_ratio=0.95, random_state=random.seed(1))\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "# 生成训练与验证集的迭代器\n",
    "train_iterator, val_iterator = data.BucketIterator.splits(\n",
    "    (train_set, val_set),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    #shuffle=True,\n",
    "    # device=device,\n",
    "    sort_within_batch=False,\n",
    "    sort_key=lambda x:len(x.text)\n",
    ")\n",
    "\n",
    "print('build dataset done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28322\n",
      "443\n"
     ]
    }
   ],
   "source": [
    "print(len(train_set))\n",
    "print(len(train_iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29813\n",
      "{'text': ['左传', '记载', '春秋', '后期', '鲁国', '大夫', '季孙氏', '家臣', '阳虎', '独掌', '权柄', '后', '标榜', '鲁国', '国君', '整肃', '跋扈', '大夫', '此举', '得不到', '知礼', '之士', '赞成', '反而', '批评', '此举', '挑战', '宗法制度', '损害', '大夫', '利益', '冲击', '天子', '权威', '不', '符合', '周礼', '次数', '阳虎', '身份', '鲁国', '大夫', '季孙氏', '家臣', '周礼', '效忠', '季孙氏', '标榜', '鲁国', '国君', '整肃', '大夫', '僭', '越', '批评', '违背', '周礼', '选择项', '宗法制度', '血缘', '核心', '故项', '与此无关', '排除', '项', '题意', '无关', '排除', '材料', '事件', '涉及', '鲁国', '国内', '周天子', '权威', '无关', '排除', '项'], 'label': [1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]}\n",
      "{'text': ['秦始皇', '统一', '六国后', '创制', '一套', '御玺', '任命', '国家', '官员', '封印', '皇帝', '之玺', '任命', '四夷', '官员', '天子', '之玺', '信玺', '用于', '国内', '四夷', '用兵', '事宜', '行玺', '皇帝', '外', '巡时', '随身携带', '材料', '皇帝', '处于', '至高无上', '地位', '秦朝', '内外', '两种', '系统', '国事', '秦朝', '实行', '中央集权', '体制', '三公九卿', '制', '提升', '行政效率', '次数', '本题', '选择', '否定', '项据', '材料', '提到', '秦始皇', '统一', '六国后', '创制', '一套', '御玺', '任命', '国家', '官员', '封印', '皇帝', '之玺', '所学', '知识', '可知', '皇帝', '处于', '至高无上', '地位', '正确', '排除', '信玺', '行玺', '区别', '秦朝', '内外', '两种', '系统', '国', '事故', '正确', '排除', '材料', '秦朝', '实行', '中央集权', '体制', '正确', '排除', '材料', '未', '涉及', '三公九卿', '制', '提升', '行政效率', '错误', '符合', '题意'], 'label': [1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]}\n"
     ]
    }
   ],
   "source": [
    "print(len(train.examples))\n",
    "print(vars(train.examples[0]))\n",
    "print(vars(train.examples[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建分类模型\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, output_size, filter_num=100, filter_size=(3,4,5), dropout=0.5):\n",
    "        '''\n",
    "        vocab_size:词典大小\n",
    "        embedding_dim:词维度大小\n",
    "        output_size:输出类别数\n",
    "        filter_num:卷积核数量\n",
    "        filter_size(3,4,5):三种卷积核，size为3,4,5，每个卷积核有filter_num个，卷积核的宽度都是embedding_dim\n",
    "        '''\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # conv2d(in_channel,out_channel,kernel_size,stride,padding),stride默认为1，padding默认为0\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, filter_num,(k, embedding_dim)) for k in filter_size])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(filter_num * len(filter_size), output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x :(batch, seq_len)\n",
    "        x = self.embedding(x) # [batch,word_num,embedding_dim] = [N,H,W]\n",
    "        x = x.unsqueeze(1) # [batch, channel, word_num, embedding_dim] = [N,C,H,W] \n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs] # len(filter_size) * (N, filter_num, H)\n",
    "        # MaxPool1d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False),stride默认为kernal_size\n",
    "        x = [F.max_pool1d(output,output.shape[2]).squeeze(2) for output in x] # len(filter_size) * (N, filter_num)\n",
    "        x = torch.cat(x, 1) # (N, filter_num * len(filter_size))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.卷积后的shape计算完整公式：\n",
    "\n",
    "input_shape:(batch,channel,height,width)\n",
    "\n",
    "$Input:(N,C,H_{in},W_{in})$\n",
    "\n",
    "$Output:(N,C,H_{out},W_{out})$\n",
    "\n",
    "$H_{out}=\\lfloor\\frac{H_{in} + 2 * padding[0] - dilation[0] * (kernel_-size[0] - 1) - 1}{stride[0]} + 1\\rfloor$\n",
    "\n",
    "$W_{out}=\\lfloor\\frac{W_{in} + 2 * padding[1] - dilation[1] * (kernel_-size[1] - 1) -1 }{stride[1]} + 1\\rfloor$\n",
    "\n",
    "2.池化max_pool1d计算完整公式：\n",
    "input_shape:(batch, channel, lin)\n",
    "\n",
    "$Input:(N,C,L_{in})$\n",
    "\n",
    "$Output:(N,C,L_{out})$\n",
    "\n",
    "$L_{out}=\\lfloor\\frac{L_{in} + 2 * padding - dilation * (kernel_-size - 1) - 1}{stride} + 1\\rfloor$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "评估\n",
    "'''\n",
    "def evaluate(model, criterion):\n",
    "    model.eval()  # 评估模型，切断dropout与batchnorm\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():  # 不更新梯度\n",
    "        for i, batch in enumerate(val_iterator):\n",
    "            train_text = batch.text  \n",
    "            train_label = batch.label\n",
    "            train_label = train_label.float()\n",
    "\n",
    "            train_text = train_text.to(DEVICE)\n",
    "            train_label = train_label.to(DEVICE)\n",
    "\n",
    "            out = model(train_text)\n",
    "            loss = criterion(out, train_label)\n",
    "            epoch_loss += float(loss.item())\n",
    "    print('evaluate loss:{}'.format(epoch_loss/len(val_iterator)))\n",
    "    \n",
    "#对所有模块和子模块进行权重初始化\n",
    "def init_weights(model):\n",
    "    for name,param in model.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter [10/300], Loss: 0.0816\n",
      "iter [10/300], Loss: 0.0827\n",
      "iter [10/300], Loss: 0.0863\n",
      "iter [10/300], Loss: 0.0817\n",
      "iter [10/300], Loss: 0.0855\n",
      "iter [10/300], Loss: 0.0837\n",
      "iter [10/300], Loss: 0.0871\n",
      "iter [10/300], Loss: 0.0844\n",
      "iter [10/300], Loss: 0.0866\n",
      "iter [10/300], Loss: 0.0859\n",
      "iter [10/300], Loss: 0.0860\n",
      "iter [10/300], Loss: 0.0777\n",
      "iter [10/300], Loss: 0.0808\n",
      "iter [10/300], Loss: 0.0890\n",
      "iter [10/300], Loss: 0.0802\n",
      "iter [10/300], Loss: 0.0814\n",
      "iter [10/300], Loss: 0.0900\n",
      "iter [10/300], Loss: 0.0843\n",
      "iter [10/300], Loss: 0.0835\n",
      "iter [10/300], Loss: 0.0880\n",
      "iter [10/300], Loss: 0.0809\n",
      "iter [10/300], Loss: 0.0843\n",
      "iter [10/300], Loss: 0.0806\n",
      "iter [10/300], Loss: 0.0874\n",
      "iter [10/300], Loss: 0.0840\n",
      "iter [10/300], Loss: 0.0824\n",
      "iter [10/300], Loss: 0.0845\n",
      "iter [10/300], Loss: 0.0798\n",
      "iter [10/300], Loss: 0.0820\n",
      "iter [10/300], Loss: 0.0855\n",
      "iter [10/300], Loss: 0.0902\n",
      "iter [10/300], Loss: 0.0861\n",
      "iter [10/300], Loss: 0.0814\n",
      "iter [10/300], Loss: 0.0894\n",
      "iter [10/300], Loss: 0.0818\n",
      "iter [10/300], Loss: 0.0808\n",
      "iter [10/300], Loss: 0.0823\n",
      "iter [10/300], Loss: 0.0868\n",
      "iter [10/300], Loss: 0.0788\n",
      "iter [10/300], Loss: 0.0835\n",
      "iter [10/300], Loss: 0.0776\n",
      "iter [10/300], Loss: 0.0834\n",
      "iter [10/300], Loss: 0.0890\n",
      "iter [10/300], Loss: 0.0850\n",
      "iter [10/300], Loss: 0.0855\n",
      "iter [10/300], Loss: 0.0802\n",
      "iter [10/300], Loss: 0.0872\n",
      "iter [10/300], Loss: 0.0823\n",
      "iter [10/300], Loss: 0.0768\n",
      "iter [10/300], Loss: 0.0828\n",
      "iter [10/300], Loss: 0.0820\n",
      "iter [10/300], Loss: 0.0869\n",
      "iter [10/300], Loss: 0.0843\n",
      "iter [10/300], Loss: 0.0815\n",
      "iter [10/300], Loss: 0.0835\n",
      "iter [10/300], Loss: 0.0893\n",
      "evaluate loss:0.07729856669902802\n",
      "iter [20/300], Loss: 0.0498\n",
      "iter [20/300], Loss: 0.0560\n",
      "iter [20/300], Loss: 0.0500\n",
      "iter [20/300], Loss: 0.0558\n",
      "iter [20/300], Loss: 0.0467\n",
      "iter [20/300], Loss: 0.0492\n",
      "iter [20/300], Loss: 0.0481\n",
      "iter [20/300], Loss: 0.0542\n",
      "iter [20/300], Loss: 0.0506\n",
      "iter [20/300], Loss: 0.0495\n",
      "iter [20/300], Loss: 0.0516\n",
      "iter [20/300], Loss: 0.0501\n",
      "iter [20/300], Loss: 0.0505\n",
      "iter [20/300], Loss: 0.0496\n",
      "iter [20/300], Loss: 0.0491\n",
      "iter [20/300], Loss: 0.0498\n",
      "iter [20/300], Loss: 0.0511\n",
      "iter [20/300], Loss: 0.0488\n",
      "iter [20/300], Loss: 0.0523\n",
      "iter [20/300], Loss: 0.0505\n",
      "iter [20/300], Loss: 0.0486\n",
      "iter [20/300], Loss: 0.0529\n",
      "iter [20/300], Loss: 0.0503\n",
      "iter [20/300], Loss: 0.0528\n",
      "iter [20/300], Loss: 0.0487\n",
      "iter [20/300], Loss: 0.0553\n",
      "iter [20/300], Loss: 0.0515\n",
      "iter [20/300], Loss: 0.0525\n",
      "iter [20/300], Loss: 0.0521\n",
      "iter [20/300], Loss: 0.0533\n",
      "iter [20/300], Loss: 0.0504\n",
      "iter [20/300], Loss: 0.0504\n",
      "iter [20/300], Loss: 0.0496\n",
      "iter [20/300], Loss: 0.0492\n",
      "iter [20/300], Loss: 0.0542\n",
      "iter [20/300], Loss: 0.0523\n",
      "iter [20/300], Loss: 0.0542\n",
      "iter [20/300], Loss: 0.0486\n",
      "iter [20/300], Loss: 0.0507\n",
      "iter [20/300], Loss: 0.0492\n",
      "iter [20/300], Loss: 0.0497\n",
      "iter [20/300], Loss: 0.0529\n",
      "iter [20/300], Loss: 0.0460\n",
      "iter [20/300], Loss: 0.0536\n",
      "iter [20/300], Loss: 0.0489\n",
      "iter [20/300], Loss: 0.0486\n",
      "iter [20/300], Loss: 0.0514\n",
      "iter [20/300], Loss: 0.0496\n",
      "iter [20/300], Loss: 0.0540\n",
      "iter [20/300], Loss: 0.0514\n",
      "iter [20/300], Loss: 0.0460\n",
      "iter [20/300], Loss: 0.0473\n",
      "iter [20/300], Loss: 0.0521\n",
      "iter [20/300], Loss: 0.0488\n",
      "iter [20/300], Loss: 0.0493\n",
      "iter [20/300], Loss: 0.0520\n",
      "evaluate loss:0.0503200056652228\n",
      "iter [30/300], Loss: 0.0402\n",
      "iter [30/300], Loss: 0.0393\n",
      "iter [30/300], Loss: 0.0370\n",
      "iter [30/300], Loss: 0.0381\n",
      "iter [30/300], Loss: 0.0413\n",
      "iter [30/300], Loss: 0.0427\n",
      "iter [30/300], Loss: 0.0368\n",
      "iter [30/300], Loss: 0.0404\n",
      "iter [30/300], Loss: 0.0382\n",
      "iter [30/300], Loss: 0.0357\n",
      "iter [30/300], Loss: 0.0383\n",
      "iter [30/300], Loss: 0.0391\n",
      "iter [30/300], Loss: 0.0413\n",
      "iter [30/300], Loss: 0.0388\n",
      "iter [30/300], Loss: 0.0401\n",
      "iter [30/300], Loss: 0.0395\n",
      "iter [30/300], Loss: 0.0390\n",
      "iter [30/300], Loss: 0.0399\n",
      "iter [30/300], Loss: 0.0397\n",
      "iter [30/300], Loss: 0.0374\n",
      "iter [30/300], Loss: 0.0361\n",
      "iter [30/300], Loss: 0.0370\n",
      "iter [30/300], Loss: 0.0380\n",
      "iter [30/300], Loss: 0.0394\n",
      "iter [30/300], Loss: 0.0361\n",
      "iter [30/300], Loss: 0.0359\n",
      "iter [30/300], Loss: 0.0405\n",
      "iter [30/300], Loss: 0.0371\n",
      "iter [30/300], Loss: 0.0393\n",
      "iter [30/300], Loss: 0.0357\n",
      "iter [30/300], Loss: 0.0379\n",
      "iter [30/300], Loss: 0.0382\n",
      "iter [30/300], Loss: 0.0387\n",
      "iter [30/300], Loss: 0.0383\n",
      "iter [30/300], Loss: 0.0372\n",
      "iter [30/300], Loss: 0.0370\n",
      "iter [30/300], Loss: 0.0400\n",
      "iter [30/300], Loss: 0.0395\n",
      "iter [30/300], Loss: 0.0398\n",
      "iter [30/300], Loss: 0.0379\n",
      "iter [30/300], Loss: 0.0379\n",
      "iter [30/300], Loss: 0.0389\n",
      "iter [30/300], Loss: 0.0365\n",
      "iter [30/300], Loss: 0.0375\n",
      "iter [30/300], Loss: 0.0380\n",
      "iter [30/300], Loss: 0.0360\n",
      "iter [30/300], Loss: 0.0403\n",
      "iter [30/300], Loss: 0.0362\n",
      "iter [30/300], Loss: 0.0352\n",
      "iter [30/300], Loss: 0.0341\n",
      "iter [30/300], Loss: 0.0386\n",
      "iter [30/300], Loss: 0.0391\n",
      "iter [30/300], Loss: 0.0342\n",
      "iter [30/300], Loss: 0.0422\n",
      "iter [30/300], Loss: 0.0345\n",
      "iter [30/300], Loss: 0.0356\n",
      "evaluate loss:0.03810598701238632\n",
      "iter [40/300], Loss: 0.0317\n",
      "iter [40/300], Loss: 0.0343\n",
      "iter [40/300], Loss: 0.0302\n",
      "iter [40/300], Loss: 0.0344\n",
      "iter [40/300], Loss: 0.0308\n",
      "iter [40/300], Loss: 0.0319\n",
      "iter [40/300], Loss: 0.0304\n",
      "iter [40/300], Loss: 0.0344\n",
      "iter [40/300], Loss: 0.0299\n",
      "iter [40/300], Loss: 0.0290\n",
      "iter [40/300], Loss: 0.0285\n",
      "iter [40/300], Loss: 0.0292\n",
      "iter [40/300], Loss: 0.0333\n",
      "iter [40/300], Loss: 0.0319\n",
      "iter [40/300], Loss: 0.0292\n",
      "iter [40/300], Loss: 0.0313\n",
      "iter [40/300], Loss: 0.0294\n",
      "iter [40/300], Loss: 0.0336\n",
      "iter [40/300], Loss: 0.0282\n",
      "iter [40/300], Loss: 0.0333\n",
      "iter [40/300], Loss: 0.0343\n",
      "iter [40/300], Loss: 0.0302\n",
      "iter [40/300], Loss: 0.0306\n",
      "iter [40/300], Loss: 0.0296\n",
      "iter [40/300], Loss: 0.0306\n",
      "iter [40/300], Loss: 0.0310\n",
      "iter [40/300], Loss: 0.0315\n",
      "iter [40/300], Loss: 0.0330\n",
      "iter [40/300], Loss: 0.0298\n",
      "iter [40/300], Loss: 0.0317\n",
      "iter [40/300], Loss: 0.0301\n",
      "iter [40/300], Loss: 0.0304\n",
      "iter [40/300], Loss: 0.0359\n",
      "iter [40/300], Loss: 0.0313\n",
      "iter [40/300], Loss: 0.0322\n",
      "iter [40/300], Loss: 0.0315\n",
      "iter [40/300], Loss: 0.0306\n",
      "iter [40/300], Loss: 0.0297\n",
      "iter [40/300], Loss: 0.0286\n",
      "iter [40/300], Loss: 0.0294\n",
      "iter [40/300], Loss: 0.0296\n",
      "iter [40/300], Loss: 0.0326\n",
      "iter [40/300], Loss: 0.0320\n",
      "iter [40/300], Loss: 0.0328\n",
      "iter [40/300], Loss: 0.0301\n",
      "iter [40/300], Loss: 0.0316\n",
      "iter [40/300], Loss: 0.0300\n",
      "iter [40/300], Loss: 0.0309\n",
      "iter [40/300], Loss: 0.0314\n",
      "iter [40/300], Loss: 0.0326\n",
      "iter [40/300], Loss: 0.0302\n",
      "iter [40/300], Loss: 0.0325\n",
      "iter [40/300], Loss: 0.0289\n",
      "iter [40/300], Loss: 0.0314\n",
      "iter [40/300], Loss: 0.0299\n",
      "iter [40/300], Loss: 0.0309\n",
      "evaluate loss:0.03150766839583715\n",
      "iter [50/300], Loss: 0.0272\n",
      "iter [50/300], Loss: 0.0292\n",
      "iter [50/300], Loss: 0.0287\n",
      "iter [50/300], Loss: 0.0281\n",
      "iter [50/300], Loss: 0.0262\n",
      "iter [50/300], Loss: 0.0271\n",
      "iter [50/300], Loss: 0.0278\n",
      "iter [50/300], Loss: 0.0287\n",
      "iter [50/300], Loss: 0.0246\n",
      "iter [50/300], Loss: 0.0273\n",
      "iter [50/300], Loss: 0.0235\n",
      "iter [50/300], Loss: 0.0266\n",
      "iter [50/300], Loss: 0.0266\n",
      "iter [50/300], Loss: 0.0286\n",
      "iter [50/300], Loss: 0.0271\n",
      "iter [50/300], Loss: 0.0266\n",
      "iter [50/300], Loss: 0.0253\n",
      "iter [50/300], Loss: 0.0266\n",
      "iter [50/300], Loss: 0.0257\n",
      "iter [50/300], Loss: 0.0293\n",
      "iter [50/300], Loss: 0.0307\n",
      "iter [50/300], Loss: 0.0266\n",
      "iter [50/300], Loss: 0.0269\n",
      "iter [50/300], Loss: 0.0261\n",
      "iter [50/300], Loss: 0.0239\n",
      "iter [50/300], Loss: 0.0266\n",
      "iter [50/300], Loss: 0.0274\n",
      "iter [50/300], Loss: 0.0247\n",
      "iter [50/300], Loss: 0.0262\n",
      "iter [50/300], Loss: 0.0256\n",
      "iter [50/300], Loss: 0.0245\n",
      "iter [50/300], Loss: 0.0265\n",
      "iter [50/300], Loss: 0.0285\n",
      "iter [50/300], Loss: 0.0270\n",
      "iter [50/300], Loss: 0.0259\n",
      "iter [50/300], Loss: 0.0249\n",
      "iter [50/300], Loss: 0.0292\n",
      "iter [50/300], Loss: 0.0276\n",
      "iter [50/300], Loss: 0.0291\n",
      "iter [50/300], Loss: 0.0281\n",
      "iter [50/300], Loss: 0.0271\n",
      "iter [50/300], Loss: 0.0267\n",
      "iter [50/300], Loss: 0.0242\n",
      "iter [50/300], Loss: 0.0270\n",
      "iter [50/300], Loss: 0.0259\n",
      "iter [50/300], Loss: 0.0240\n",
      "iter [50/300], Loss: 0.0258\n",
      "iter [50/300], Loss: 0.0257\n",
      "iter [50/300], Loss: 0.0293\n",
      "iter [50/300], Loss: 0.0284\n",
      "iter [50/300], Loss: 0.0275\n",
      "iter [50/300], Loss: 0.0259\n",
      "iter [50/300], Loss: 0.0272\n",
      "iter [50/300], Loss: 0.0249\n",
      "iter [50/300], Loss: 0.0252\n",
      "iter [50/300], Loss: 0.0236\n",
      "evaluate loss:0.027390899136662483\n",
      "iter [60/300], Loss: 0.0185\n",
      "iter [60/300], Loss: 0.0217\n",
      "iter [60/300], Loss: 0.0239\n",
      "iter [60/300], Loss: 0.0234\n",
      "iter [60/300], Loss: 0.0228\n",
      "iter [60/300], Loss: 0.0236\n",
      "iter [60/300], Loss: 0.0271\n",
      "iter [60/300], Loss: 0.0239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter [60/300], Loss: 0.0224\n",
      "iter [60/300], Loss: 0.0256\n",
      "iter [60/300], Loss: 0.0252\n",
      "iter [60/300], Loss: 0.0227\n",
      "iter [60/300], Loss: 0.0241\n",
      "iter [60/300], Loss: 0.0217\n",
      "iter [60/300], Loss: 0.0257\n",
      "iter [60/300], Loss: 0.0230\n",
      "iter [60/300], Loss: 0.0252\n",
      "iter [60/300], Loss: 0.0248\n",
      "iter [60/300], Loss: 0.0237\n",
      "iter [60/300], Loss: 0.0242\n",
      "iter [60/300], Loss: 0.0225\n",
      "iter [60/300], Loss: 0.0220\n",
      "iter [60/300], Loss: 0.0242\n",
      "iter [60/300], Loss: 0.0242\n",
      "iter [60/300], Loss: 0.0239\n",
      "iter [60/300], Loss: 0.0231\n",
      "iter [60/300], Loss: 0.0243\n",
      "iter [60/300], Loss: 0.0245\n",
      "iter [60/300], Loss: 0.0229\n",
      "iter [60/300], Loss: 0.0269\n",
      "iter [60/300], Loss: 0.0247\n",
      "iter [60/300], Loss: 0.0227\n",
      "iter [60/300], Loss: 0.0228\n",
      "iter [60/300], Loss: 0.0232\n",
      "iter [60/300], Loss: 0.0242\n",
      "iter [60/300], Loss: 0.0231\n",
      "iter [60/300], Loss: 0.0205\n",
      "iter [60/300], Loss: 0.0234\n",
      "iter [60/300], Loss: 0.0250\n",
      "iter [60/300], Loss: 0.0220\n",
      "iter [60/300], Loss: 0.0226\n",
      "iter [60/300], Loss: 0.0245\n",
      "iter [60/300], Loss: 0.0221\n",
      "iter [60/300], Loss: 0.0242\n",
      "iter [60/300], Loss: 0.0242\n",
      "iter [60/300], Loss: 0.0238\n",
      "iter [60/300], Loss: 0.0256\n",
      "iter [60/300], Loss: 0.0239\n",
      "iter [60/300], Loss: 0.0226\n",
      "iter [60/300], Loss: 0.0238\n",
      "iter [60/300], Loss: 0.0229\n",
      "iter [60/300], Loss: 0.0230\n",
      "iter [60/300], Loss: 0.0254\n",
      "iter [60/300], Loss: 0.0227\n",
      "iter [60/300], Loss: 0.0239\n",
      "iter [60/300], Loss: 0.0222\n",
      "evaluate loss:0.02453434591492017\n",
      "iter [70/300], Loss: 0.0225\n",
      "iter [70/300], Loss: 0.0204\n",
      "iter [70/300], Loss: 0.0191\n",
      "iter [70/300], Loss: 0.0178\n",
      "iter [70/300], Loss: 0.0224\n",
      "iter [70/300], Loss: 0.0241\n",
      "iter [70/300], Loss: 0.0234\n",
      "iter [70/300], Loss: 0.0216\n",
      "iter [70/300], Loss: 0.0241\n",
      "iter [70/300], Loss: 0.0215\n",
      "iter [70/300], Loss: 0.0242\n",
      "iter [70/300], Loss: 0.0221\n",
      "iter [70/300], Loss: 0.0225\n",
      "iter [70/300], Loss: 0.0203\n",
      "iter [70/300], Loss: 0.0214\n",
      "iter [70/300], Loss: 0.0201\n",
      "iter [70/300], Loss: 0.0222\n",
      "iter [70/300], Loss: 0.0218\n",
      "iter [70/300], Loss: 0.0206\n",
      "iter [70/300], Loss: 0.0210\n",
      "iter [70/300], Loss: 0.0211\n",
      "iter [70/300], Loss: 0.0228\n",
      "iter [70/300], Loss: 0.0196\n",
      "iter [70/300], Loss: 0.0208\n",
      "iter [70/300], Loss: 0.0195\n",
      "iter [70/300], Loss: 0.0199\n",
      "iter [70/300], Loss: 0.0189\n",
      "iter [70/300], Loss: 0.0222\n",
      "iter [70/300], Loss: 0.0212\n",
      "iter [70/300], Loss: 0.0205\n",
      "iter [70/300], Loss: 0.0210\n",
      "iter [70/300], Loss: 0.0226\n",
      "iter [70/300], Loss: 0.0202\n",
      "iter [70/300], Loss: 0.0234\n",
      "iter [70/300], Loss: 0.0219\n",
      "iter [70/300], Loss: 0.0203\n",
      "iter [70/300], Loss: 0.0223\n",
      "iter [70/300], Loss: 0.0237\n",
      "iter [70/300], Loss: 0.0210\n",
      "iter [70/300], Loss: 0.0203\n",
      "iter [70/300], Loss: 0.0213\n",
      "iter [70/300], Loss: 0.0214\n",
      "iter [70/300], Loss: 0.0208\n",
      "iter [70/300], Loss: 0.0217\n",
      "iter [70/300], Loss: 0.0206\n",
      "iter [70/300], Loss: 0.0209\n",
      "iter [70/300], Loss: 0.0207\n",
      "iter [70/300], Loss: 0.0225\n",
      "iter [70/300], Loss: 0.0204\n",
      "iter [70/300], Loss: 0.0208\n",
      "iter [70/300], Loss: 0.0224\n",
      "iter [70/300], Loss: 0.0218\n",
      "iter [70/300], Loss: 0.0193\n",
      "iter [70/300], Loss: 0.0202\n",
      "iter [70/300], Loss: 0.0207\n",
      "iter [70/300], Loss: 0.0201\n",
      "evaluate loss:0.02243736758828163\n",
      "iter [80/300], Loss: 0.0202\n",
      "iter [80/300], Loss: 0.0189\n",
      "iter [80/300], Loss: 0.0191\n",
      "iter [80/300], Loss: 0.0209\n",
      "iter [80/300], Loss: 0.0199\n",
      "iter [80/300], Loss: 0.0178\n",
      "iter [80/300], Loss: 0.0179\n",
      "iter [80/300], Loss: 0.0174\n",
      "iter [80/300], Loss: 0.0228\n",
      "iter [80/300], Loss: 0.0193\n",
      "iter [80/300], Loss: 0.0206\n",
      "iter [80/300], Loss: 0.0210\n",
      "iter [80/300], Loss: 0.0200\n",
      "iter [80/300], Loss: 0.0205\n",
      "iter [80/300], Loss: 0.0199\n",
      "iter [80/300], Loss: 0.0185\n",
      "iter [80/300], Loss: 0.0187\n",
      "iter [80/300], Loss: 0.0205\n",
      "iter [80/300], Loss: 0.0191\n",
      "iter [80/300], Loss: 0.0206\n",
      "iter [80/300], Loss: 0.0184\n",
      "iter [80/300], Loss: 0.0191\n",
      "iter [80/300], Loss: 0.0175\n",
      "iter [80/300], Loss: 0.0203\n",
      "iter [80/300], Loss: 0.0194\n",
      "iter [80/300], Loss: 0.0199\n",
      "iter [80/300], Loss: 0.0188\n",
      "iter [80/300], Loss: 0.0190\n",
      "iter [80/300], Loss: 0.0197\n",
      "iter [80/300], Loss: 0.0184\n",
      "iter [80/300], Loss: 0.0193\n",
      "iter [80/300], Loss: 0.0173\n",
      "iter [80/300], Loss: 0.0201\n",
      "iter [80/300], Loss: 0.0194\n",
      "iter [80/300], Loss: 0.0188\n",
      "iter [80/300], Loss: 0.0206\n",
      "iter [80/300], Loss: 0.0211\n",
      "iter [80/300], Loss: 0.0194\n",
      "iter [80/300], Loss: 0.0185\n",
      "iter [80/300], Loss: 0.0185\n",
      "iter [80/300], Loss: 0.0182\n",
      "iter [80/300], Loss: 0.0259\n",
      "iter [80/300], Loss: 0.0191\n",
      "iter [80/300], Loss: 0.0208\n",
      "iter [80/300], Loss: 0.0190\n",
      "iter [80/300], Loss: 0.0194\n",
      "iter [80/300], Loss: 0.0197\n",
      "iter [80/300], Loss: 0.0188\n",
      "iter [80/300], Loss: 0.0208\n",
      "iter [80/300], Loss: 0.0188\n",
      "iter [80/300], Loss: 0.0179\n",
      "iter [80/300], Loss: 0.0187\n",
      "iter [80/300], Loss: 0.0207\n",
      "iter [80/300], Loss: 0.0196\n",
      "iter [80/300], Loss: 0.0199\n",
      "iter [80/300], Loss: 0.0207\n",
      "evaluate loss:0.020886678248643875\n",
      "iter [90/300], Loss: 0.0178\n",
      "iter [90/300], Loss: 0.0178\n",
      "iter [90/300], Loss: 0.0168\n",
      "iter [90/300], Loss: 0.0181\n",
      "iter [90/300], Loss: 0.0179\n",
      "iter [90/300], Loss: 0.0208\n",
      "iter [90/300], Loss: 0.0176\n",
      "iter [90/300], Loss: 0.0193\n",
      "iter [90/300], Loss: 0.0190\n",
      "iter [90/300], Loss: 0.0188\n",
      "iter [90/300], Loss: 0.0182\n",
      "iter [90/300], Loss: 0.0199\n",
      "iter [90/300], Loss: 0.0183\n",
      "iter [90/300], Loss: 0.0193\n",
      "iter [90/300], Loss: 0.0173\n",
      "iter [90/300], Loss: 0.0189\n",
      "iter [90/300], Loss: 0.0185\n",
      "iter [90/300], Loss: 0.0162\n",
      "iter [90/300], Loss: 0.0178\n",
      "iter [90/300], Loss: 0.0163\n",
      "iter [90/300], Loss: 0.0188\n",
      "iter [90/300], Loss: 0.0165\n",
      "iter [90/300], Loss: 0.0169\n",
      "iter [90/300], Loss: 0.0196\n",
      "iter [90/300], Loss: 0.0169\n",
      "iter [90/300], Loss: 0.0165\n",
      "iter [90/300], Loss: 0.0160\n",
      "iter [90/300], Loss: 0.0165\n",
      "iter [90/300], Loss: 0.0177\n",
      "iter [90/300], Loss: 0.0189\n",
      "iter [90/300], Loss: 0.0156\n",
      "iter [90/300], Loss: 0.0177\n",
      "iter [90/300], Loss: 0.0174\n",
      "iter [90/300], Loss: 0.0178\n",
      "iter [90/300], Loss: 0.0182\n",
      "iter [90/300], Loss: 0.0187\n",
      "iter [90/300], Loss: 0.0186\n",
      "iter [90/300], Loss: 0.0182\n",
      "iter [90/300], Loss: 0.0197\n",
      "iter [90/300], Loss: 0.0177\n",
      "iter [90/300], Loss: 0.0189\n",
      "iter [90/300], Loss: 0.0187\n",
      "iter [90/300], Loss: 0.0162\n",
      "iter [90/300], Loss: 0.0188\n",
      "iter [90/300], Loss: 0.0180\n",
      "iter [90/300], Loss: 0.0175\n",
      "iter [90/300], Loss: 0.0177\n",
      "iter [90/300], Loss: 0.0178\n",
      "iter [90/300], Loss: 0.0152\n",
      "iter [90/300], Loss: 0.0226\n",
      "iter [90/300], Loss: 0.0178\n",
      "iter [90/300], Loss: 0.0167\n",
      "iter [90/300], Loss: 0.0196\n",
      "iter [90/300], Loss: 0.0172\n",
      "iter [90/300], Loss: 0.0193\n",
      "iter [90/300], Loss: 0.0165\n",
      "evaluate loss:0.01968240427474181\n",
      "iter [100/300], Loss: 0.0185\n",
      "iter [100/300], Loss: 0.0181\n",
      "iter [100/300], Loss: 0.0151\n",
      "iter [100/300], Loss: 0.0165\n",
      "iter [100/300], Loss: 0.0158\n",
      "iter [100/300], Loss: 0.0170\n",
      "iter [100/300], Loss: 0.0160\n",
      "iter [100/300], Loss: 0.0177\n",
      "iter [100/300], Loss: 0.0166\n",
      "iter [100/300], Loss: 0.0160\n",
      "iter [100/300], Loss: 0.0160\n",
      "iter [100/300], Loss: 0.0165\n",
      "iter [100/300], Loss: 0.0155\n",
      "iter [100/300], Loss: 0.0163\n",
      "iter [100/300], Loss: 0.0182\n",
      "iter [100/300], Loss: 0.0156\n",
      "iter [100/300], Loss: 0.0163\n",
      "iter [100/300], Loss: 0.0160\n",
      "iter [100/300], Loss: 0.0193\n",
      "iter [100/300], Loss: 0.0160\n",
      "iter [100/300], Loss: 0.0171\n",
      "iter [100/300], Loss: 0.0191\n",
      "iter [100/300], Loss: 0.0185\n",
      "iter [100/300], Loss: 0.0160\n",
      "iter [100/300], Loss: 0.0162\n",
      "iter [100/300], Loss: 0.0168\n",
      "iter [100/300], Loss: 0.0164\n",
      "iter [100/300], Loss: 0.0164\n",
      "iter [100/300], Loss: 0.0176\n",
      "iter [100/300], Loss: 0.0178\n",
      "iter [100/300], Loss: 0.0160\n",
      "iter [100/300], Loss: 0.0166\n",
      "iter [100/300], Loss: 0.0168\n",
      "iter [100/300], Loss: 0.0193\n",
      "iter [100/300], Loss: 0.0156\n",
      "iter [100/300], Loss: 0.0173\n",
      "iter [100/300], Loss: 0.0160\n",
      "iter [100/300], Loss: 0.0172\n",
      "iter [100/300], Loss: 0.0164\n",
      "iter [100/300], Loss: 0.0154\n",
      "iter [100/300], Loss: 0.0180\n",
      "iter [100/300], Loss: 0.0145\n",
      "iter [100/300], Loss: 0.0148\n",
      "iter [100/300], Loss: 0.0156\n",
      "iter [100/300], Loss: 0.0178\n",
      "iter [100/300], Loss: 0.0173\n",
      "iter [100/300], Loss: 0.0152\n",
      "iter [100/300], Loss: 0.0175\n",
      "iter [100/300], Loss: 0.0172\n",
      "iter [100/300], Loss: 0.0183\n",
      "iter [100/300], Loss: 0.0163\n",
      "iter [100/300], Loss: 0.0163\n",
      "iter [100/300], Loss: 0.0173\n",
      "iter [100/300], Loss: 0.0175\n",
      "iter [100/300], Loss: 0.0155\n",
      "iter [100/300], Loss: 0.0163\n",
      "evaluate loss:0.01865180240323146\n",
      "iter [110/300], Loss: 0.0161\n",
      "iter [110/300], Loss: 0.0149\n",
      "iter [110/300], Loss: 0.0134\n",
      "iter [110/300], Loss: 0.0165\n",
      "iter [110/300], Loss: 0.0146\n",
      "iter [110/300], Loss: 0.0157\n",
      "iter [110/300], Loss: 0.0154\n",
      "iter [110/300], Loss: 0.0160\n",
      "iter [110/300], Loss: 0.0181\n",
      "iter [110/300], Loss: 0.0156\n",
      "iter [110/300], Loss: 0.0152\n",
      "iter [110/300], Loss: 0.0143\n",
      "iter [110/300], Loss: 0.0145\n",
      "iter [110/300], Loss: 0.0134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter [110/300], Loss: 0.0147\n",
      "iter [110/300], Loss: 0.0155\n",
      "iter [110/300], Loss: 0.0151\n",
      "iter [110/300], Loss: 0.0147\n",
      "iter [110/300], Loss: 0.0145\n",
      "iter [110/300], Loss: 0.0149\n",
      "iter [110/300], Loss: 0.0149\n",
      "iter [110/300], Loss: 0.0133\n",
      "iter [110/300], Loss: 0.0140\n",
      "iter [110/300], Loss: 0.0137\n",
      "iter [110/300], Loss: 0.0171\n",
      "iter [110/300], Loss: 0.0141\n",
      "iter [110/300], Loss: 0.0164\n",
      "iter [110/300], Loss: 0.0162\n",
      "iter [110/300], Loss: 0.0151\n",
      "iter [110/300], Loss: 0.0149\n",
      "iter [110/300], Loss: 0.0174\n",
      "iter [110/300], Loss: 0.0175\n",
      "iter [110/300], Loss: 0.0174\n",
      "iter [110/300], Loss: 0.0169\n",
      "iter [110/300], Loss: 0.0184\n",
      "iter [110/300], Loss: 0.0162\n",
      "iter [110/300], Loss: 0.0140\n",
      "iter [110/300], Loss: 0.0168\n",
      "iter [110/300], Loss: 0.0164\n",
      "iter [110/300], Loss: 0.0156\n",
      "iter [110/300], Loss: 0.0149\n",
      "iter [110/300], Loss: 0.0148\n",
      "iter [110/300], Loss: 0.0168\n",
      "iter [110/300], Loss: 0.0158\n",
      "iter [110/300], Loss: 0.0159\n",
      "iter [110/300], Loss: 0.0174\n",
      "iter [110/300], Loss: 0.0159\n",
      "iter [110/300], Loss: 0.0168\n",
      "iter [110/300], Loss: 0.0155\n",
      "iter [110/300], Loss: 0.0146\n",
      "iter [110/300], Loss: 0.0158\n",
      "iter [110/300], Loss: 0.0167\n",
      "iter [110/300], Loss: 0.0146\n",
      "iter [110/300], Loss: 0.0163\n",
      "iter [110/300], Loss: 0.0164\n",
      "iter [110/300], Loss: 0.0167\n",
      "evaluate loss:0.017803600368400414\n",
      "iter [120/300], Loss: 0.0152\n",
      "iter [120/300], Loss: 0.0138\n",
      "iter [120/300], Loss: 0.0143\n",
      "iter [120/300], Loss: 0.0177\n",
      "iter [120/300], Loss: 0.0171\n",
      "iter [120/300], Loss: 0.0153\n",
      "iter [120/300], Loss: 0.0120\n",
      "iter [120/300], Loss: 0.0156\n",
      "iter [120/300], Loss: 0.0130\n",
      "iter [120/300], Loss: 0.0143\n",
      "iter [120/300], Loss: 0.0147\n",
      "iter [120/300], Loss: 0.0141\n",
      "iter [120/300], Loss: 0.0121\n",
      "iter [120/300], Loss: 0.0136\n",
      "iter [120/300], Loss: 0.0152\n",
      "iter [120/300], Loss: 0.0136\n",
      "iter [120/300], Loss: 0.0153\n",
      "iter [120/300], Loss: 0.0161\n",
      "iter [120/300], Loss: 0.0150\n",
      "iter [120/300], Loss: 0.0140\n",
      "iter [120/300], Loss: 0.0155\n",
      "iter [120/300], Loss: 0.0151\n",
      "iter [120/300], Loss: 0.0151\n",
      "iter [120/300], Loss: 0.0148\n",
      "iter [120/300], Loss: 0.0143\n",
      "iter [120/300], Loss: 0.0149\n",
      "iter [120/300], Loss: 0.0152\n",
      "iter [120/300], Loss: 0.0133\n",
      "iter [120/300], Loss: 0.0146\n",
      "iter [120/300], Loss: 0.0149\n",
      "iter [120/300], Loss: 0.0170\n",
      "iter [120/300], Loss: 0.0142\n",
      "iter [120/300], Loss: 0.0135\n",
      "iter [120/300], Loss: 0.0149\n",
      "iter [120/300], Loss: 0.0140\n",
      "iter [120/300], Loss: 0.0153\n",
      "iter [120/300], Loss: 0.0161\n",
      "iter [120/300], Loss: 0.0125\n",
      "iter [120/300], Loss: 0.0149\n",
      "iter [120/300], Loss: 0.0152\n",
      "iter [120/300], Loss: 0.0135\n",
      "iter [120/300], Loss: 0.0155\n",
      "iter [120/300], Loss: 0.0129\n",
      "iter [120/300], Loss: 0.0155\n",
      "iter [120/300], Loss: 0.0144\n",
      "iter [120/300], Loss: 0.0152\n",
      "iter [120/300], Loss: 0.0156\n",
      "iter [120/300], Loss: 0.0150\n",
      "iter [120/300], Loss: 0.0164\n",
      "iter [120/300], Loss: 0.0141\n",
      "iter [120/300], Loss: 0.0146\n",
      "iter [120/300], Loss: 0.0147\n",
      "iter [120/300], Loss: 0.0139\n",
      "iter [120/300], Loss: 0.0141\n",
      "iter [120/300], Loss: 0.0148\n",
      "iter [120/300], Loss: 0.0131\n",
      "evaluate loss:0.01705935162802537\n",
      "iter [130/300], Loss: 0.0133\n",
      "iter [130/300], Loss: 0.0120\n",
      "iter [130/300], Loss: 0.0146\n",
      "iter [130/300], Loss: 0.0138\n",
      "iter [130/300], Loss: 0.0136\n",
      "iter [130/300], Loss: 0.0147\n",
      "iter [130/300], Loss: 0.0129\n",
      "iter [130/300], Loss: 0.0131\n",
      "iter [130/300], Loss: 0.0120\n",
      "iter [130/300], Loss: 0.0141\n",
      "iter [130/300], Loss: 0.0136\n",
      "iter [130/300], Loss: 0.0135\n",
      "iter [130/300], Loss: 0.0131\n",
      "iter [130/300], Loss: 0.0140\n",
      "iter [130/300], Loss: 0.0149\n",
      "iter [130/300], Loss: 0.0149\n",
      "iter [130/300], Loss: 0.0143\n",
      "iter [130/300], Loss: 0.0119\n",
      "iter [130/300], Loss: 0.0125\n",
      "iter [130/300], Loss: 0.0142\n",
      "iter [130/300], Loss: 0.0138\n",
      "iter [130/300], Loss: 0.0131\n",
      "iter [130/300], Loss: 0.0135\n",
      "iter [130/300], Loss: 0.0153\n",
      "iter [130/300], Loss: 0.0129\n",
      "iter [130/300], Loss: 0.0142\n",
      "iter [130/300], Loss: 0.0136\n",
      "iter [130/300], Loss: 0.0156\n",
      "iter [130/300], Loss: 0.0131\n",
      "iter [130/300], Loss: 0.0129\n",
      "iter [130/300], Loss: 0.0125\n",
      "iter [130/300], Loss: 0.0162\n",
      "iter [130/300], Loss: 0.0112\n",
      "iter [130/300], Loss: 0.0133\n",
      "iter [130/300], Loss: 0.0146\n",
      "iter [130/300], Loss: 0.0138\n",
      "iter [130/300], Loss: 0.0158\n",
      "iter [130/300], Loss: 0.0153\n",
      "iter [130/300], Loss: 0.0144\n",
      "iter [130/300], Loss: 0.0133\n",
      "iter [130/300], Loss: 0.0153\n",
      "iter [130/300], Loss: 0.0132\n",
      "iter [130/300], Loss: 0.0146\n",
      "iter [130/300], Loss: 0.0130\n",
      "iter [130/300], Loss: 0.0150\n",
      "iter [130/300], Loss: 0.0137\n",
      "iter [130/300], Loss: 0.0129\n",
      "iter [130/300], Loss: 0.0125\n",
      "iter [130/300], Loss: 0.0137\n",
      "iter [130/300], Loss: 0.0146\n",
      "iter [130/300], Loss: 0.0132\n",
      "iter [130/300], Loss: 0.0133\n",
      "iter [130/300], Loss: 0.0157\n",
      "iter [130/300], Loss: 0.0119\n",
      "iter [130/300], Loss: 0.0130\n",
      "iter [130/300], Loss: 0.0163\n",
      "evaluate loss:0.016469485747317474\n",
      "iter [140/300], Loss: 0.0133\n",
      "iter [140/300], Loss: 0.0141\n",
      "iter [140/300], Loss: 0.0133\n",
      "iter [140/300], Loss: 0.0126\n",
      "iter [140/300], Loss: 0.0145\n",
      "iter [140/300], Loss: 0.0145\n",
      "iter [140/300], Loss: 0.0138\n",
      "iter [140/300], Loss: 0.0137\n",
      "iter [140/300], Loss: 0.0126\n",
      "iter [140/300], Loss: 0.0124\n",
      "iter [140/300], Loss: 0.0144\n",
      "iter [140/300], Loss: 0.0139\n",
      "iter [140/300], Loss: 0.0133\n",
      "iter [140/300], Loss: 0.0117\n",
      "iter [140/300], Loss: 0.0131\n",
      "iter [140/300], Loss: 0.0138\n",
      "iter [140/300], Loss: 0.0141\n",
      "iter [140/300], Loss: 0.0110\n",
      "iter [140/300], Loss: 0.0143\n",
      "iter [140/300], Loss: 0.0111\n",
      "iter [140/300], Loss: 0.0138\n",
      "iter [140/300], Loss: 0.0115\n",
      "iter [140/300], Loss: 0.0122\n",
      "iter [140/300], Loss: 0.0141\n",
      "iter [140/300], Loss: 0.0132\n",
      "iter [140/300], Loss: 0.0132\n",
      "iter [140/300], Loss: 0.0146\n",
      "iter [140/300], Loss: 0.0120\n",
      "iter [140/300], Loss: 0.0111\n",
      "iter [140/300], Loss: 0.0124\n",
      "iter [140/300], Loss: 0.0134\n",
      "iter [140/300], Loss: 0.0138\n",
      "iter [140/300], Loss: 0.0121\n",
      "iter [140/300], Loss: 0.0123\n",
      "iter [140/300], Loss: 0.0133\n",
      "iter [140/300], Loss: 0.0105\n",
      "iter [140/300], Loss: 0.0134\n",
      "iter [140/300], Loss: 0.0126\n",
      "iter [140/300], Loss: 0.0126\n",
      "iter [140/300], Loss: 0.0148\n",
      "iter [140/300], Loss: 0.0128\n",
      "iter [140/300], Loss: 0.0138\n",
      "iter [140/300], Loss: 0.0121\n",
      "iter [140/300], Loss: 0.0136\n",
      "iter [140/300], Loss: 0.0130\n",
      "iter [140/300], Loss: 0.0129\n",
      "iter [140/300], Loss: 0.0130\n",
      "iter [140/300], Loss: 0.0127\n",
      "iter [140/300], Loss: 0.0150\n",
      "iter [140/300], Loss: 0.0118\n",
      "iter [140/300], Loss: 0.0135\n",
      "iter [140/300], Loss: 0.0110\n",
      "iter [140/300], Loss: 0.0136\n",
      "iter [140/300], Loss: 0.0119\n",
      "iter [140/300], Loss: 0.0136\n",
      "iter [140/300], Loss: 0.0122\n",
      "evaluate loss:0.015937790585060913\n",
      "iter [150/300], Loss: 0.0128\n",
      "iter [150/300], Loss: 0.0117\n",
      "iter [150/300], Loss: 0.0122\n",
      "iter [150/300], Loss: 0.0125\n",
      "iter [150/300], Loss: 0.0117\n",
      "iter [150/300], Loss: 0.0131\n",
      "iter [150/300], Loss: 0.0105\n",
      "iter [150/300], Loss: 0.0118\n",
      "iter [150/300], Loss: 0.0124\n",
      "iter [150/300], Loss: 0.0114\n",
      "iter [150/300], Loss: 0.0137\n",
      "iter [150/300], Loss: 0.0127\n",
      "iter [150/300], Loss: 0.0116\n",
      "iter [150/300], Loss: 0.0128\n",
      "iter [150/300], Loss: 0.0131\n",
      "iter [150/300], Loss: 0.0125\n",
      "iter [150/300], Loss: 0.0123\n",
      "iter [150/300], Loss: 0.0123\n",
      "iter [150/300], Loss: 0.0119\n",
      "iter [150/300], Loss: 0.0115\n",
      "iter [150/300], Loss: 0.0147\n",
      "iter [150/300], Loss: 0.0115\n",
      "iter [150/300], Loss: 0.0121\n",
      "iter [150/300], Loss: 0.0128\n",
      "iter [150/300], Loss: 0.0132\n",
      "iter [150/300], Loss: 0.0115\n",
      "iter [150/300], Loss: 0.0126\n",
      "iter [150/300], Loss: 0.0116\n",
      "iter [150/300], Loss: 0.0121\n",
      "iter [150/300], Loss: 0.0131\n",
      "iter [150/300], Loss: 0.0110\n",
      "iter [150/300], Loss: 0.0130\n",
      "iter [150/300], Loss: 0.0138\n",
      "iter [150/300], Loss: 0.0119\n",
      "iter [150/300], Loss: 0.0113\n",
      "iter [150/300], Loss: 0.0127\n",
      "iter [150/300], Loss: 0.0125\n",
      "iter [150/300], Loss: 0.0124\n",
      "iter [150/300], Loss: 0.0124\n",
      "iter [150/300], Loss: 0.0117\n",
      "iter [150/300], Loss: 0.0123\n",
      "iter [150/300], Loss: 0.0119\n",
      "iter [150/300], Loss: 0.0109\n",
      "iter [150/300], Loss: 0.0131\n",
      "iter [150/300], Loss: 0.0135\n",
      "iter [150/300], Loss: 0.0126\n",
      "iter [150/300], Loss: 0.0111\n",
      "iter [150/300], Loss: 0.0135\n",
      "iter [150/300], Loss: 0.0129\n",
      "iter [150/300], Loss: 0.0120\n",
      "iter [150/300], Loss: 0.0119\n",
      "iter [150/300], Loss: 0.0127\n",
      "iter [150/300], Loss: 0.0136\n",
      "iter [150/300], Loss: 0.0121\n",
      "iter [150/300], Loss: 0.0126\n",
      "iter [150/300], Loss: 0.0126\n",
      "evaluate loss:0.015474789775907993\n",
      "iter [160/300], Loss: 0.0119\n",
      "iter [160/300], Loss: 0.0121\n",
      "iter [160/300], Loss: 0.0116\n",
      "iter [160/300], Loss: 0.0110\n",
      "iter [160/300], Loss: 0.0112\n",
      "iter [160/300], Loss: 0.0132\n",
      "iter [160/300], Loss: 0.0114\n",
      "iter [160/300], Loss: 0.0107\n",
      "iter [160/300], Loss: 0.0117\n",
      "iter [160/300], Loss: 0.0126\n",
      "iter [160/300], Loss: 0.0112\n",
      "iter [160/300], Loss: 0.0115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter [160/300], Loss: 0.0104\n",
      "iter [160/300], Loss: 0.0123\n",
      "iter [160/300], Loss: 0.0104\n",
      "iter [160/300], Loss: 0.0146\n",
      "iter [160/300], Loss: 0.0128\n",
      "iter [160/300], Loss: 0.0119\n",
      "iter [160/300], Loss: 0.0142\n",
      "iter [160/300], Loss: 0.0114\n",
      "iter [160/300], Loss: 0.0121\n",
      "iter [160/300], Loss: 0.0101\n",
      "iter [160/300], Loss: 0.0117\n",
      "iter [160/300], Loss: 0.0102\n",
      "iter [160/300], Loss: 0.0113\n",
      "iter [160/300], Loss: 0.0112\n",
      "iter [160/300], Loss: 0.0116\n",
      "iter [160/300], Loss: 0.0121\n",
      "iter [160/300], Loss: 0.0130\n",
      "iter [160/300], Loss: 0.0111\n",
      "iter [160/300], Loss: 0.0128\n",
      "iter [160/300], Loss: 0.0113\n",
      "iter [160/300], Loss: 0.0129\n",
      "iter [160/300], Loss: 0.0124\n",
      "iter [160/300], Loss: 0.0114\n",
      "iter [160/300], Loss: 0.0099\n",
      "iter [160/300], Loss: 0.0126\n",
      "iter [160/300], Loss: 0.0107\n",
      "iter [160/300], Loss: 0.0113\n",
      "iter [160/300], Loss: 0.0135\n",
      "iter [160/300], Loss: 0.0126\n",
      "iter [160/300], Loss: 0.0101\n",
      "iter [160/300], Loss: 0.0121\n",
      "iter [160/300], Loss: 0.0121\n",
      "iter [160/300], Loss: 0.0117\n",
      "iter [160/300], Loss: 0.0101\n",
      "iter [160/300], Loss: 0.0120\n",
      "iter [160/300], Loss: 0.0118\n",
      "iter [160/300], Loss: 0.0106\n",
      "iter [160/300], Loss: 0.0121\n",
      "iter [160/300], Loss: 0.0118\n",
      "iter [160/300], Loss: 0.0125\n",
      "iter [160/300], Loss: 0.0123\n",
      "iter [160/300], Loss: 0.0113\n",
      "iter [160/300], Loss: 0.0119\n",
      "iter [160/300], Loss: 0.0120\n",
      "evaluate loss:0.015093798749148846\n",
      "iter [170/300], Loss: 0.0110\n",
      "iter [170/300], Loss: 0.0127\n",
      "iter [170/300], Loss: 0.0106\n",
      "iter [170/300], Loss: 0.0125\n",
      "iter [170/300], Loss: 0.0109\n",
      "iter [170/300], Loss: 0.0105\n",
      "iter [170/300], Loss: 0.0100\n",
      "iter [170/300], Loss: 0.0114\n",
      "iter [170/300], Loss: 0.0101\n",
      "iter [170/300], Loss: 0.0122\n",
      "iter [170/300], Loss: 0.0126\n",
      "iter [170/300], Loss: 0.0109\n",
      "iter [170/300], Loss: 0.0114\n",
      "iter [170/300], Loss: 0.0097\n",
      "iter [170/300], Loss: 0.0118\n",
      "iter [170/300], Loss: 0.0100\n",
      "iter [170/300], Loss: 0.0097\n",
      "iter [170/300], Loss: 0.0104\n",
      "iter [170/300], Loss: 0.0103\n",
      "iter [170/300], Loss: 0.0101\n",
      "iter [170/300], Loss: 0.0100\n",
      "iter [170/300], Loss: 0.0119\n",
      "iter [170/300], Loss: 0.0107\n",
      "iter [170/300], Loss: 0.0105\n",
      "iter [170/300], Loss: 0.0106\n",
      "iter [170/300], Loss: 0.0110\n",
      "iter [170/300], Loss: 0.0113\n",
      "iter [170/300], Loss: 0.0120\n",
      "iter [170/300], Loss: 0.0117\n",
      "iter [170/300], Loss: 0.0108\n",
      "iter [170/300], Loss: 0.0106\n",
      "iter [170/300], Loss: 0.0107\n",
      "iter [170/300], Loss: 0.0118\n",
      "iter [170/300], Loss: 0.0108\n",
      "iter [170/300], Loss: 0.0109\n",
      "iter [170/300], Loss: 0.0107\n",
      "iter [170/300], Loss: 0.0110\n",
      "iter [170/300], Loss: 0.0123\n",
      "iter [170/300], Loss: 0.0099\n",
      "iter [170/300], Loss: 0.0132\n",
      "iter [170/300], Loss: 0.0109\n",
      "iter [170/300], Loss: 0.0127\n",
      "iter [170/300], Loss: 0.0105\n",
      "iter [170/300], Loss: 0.0111\n",
      "iter [170/300], Loss: 0.0117\n",
      "iter [170/300], Loss: 0.0129\n",
      "iter [170/300], Loss: 0.0118\n",
      "iter [170/300], Loss: 0.0105\n",
      "iter [170/300], Loss: 0.0106\n",
      "iter [170/300], Loss: 0.0118\n",
      "iter [170/300], Loss: 0.0107\n",
      "iter [170/300], Loss: 0.0117\n",
      "iter [170/300], Loss: 0.0135\n",
      "iter [170/300], Loss: 0.0117\n",
      "iter [170/300], Loss: 0.0100\n",
      "iter [170/300], Loss: 0.0106\n",
      "evaluate loss:0.014659930331011614\n",
      "iter [180/300], Loss: 0.0115\n",
      "iter [180/300], Loss: 0.0092\n",
      "iter [180/300], Loss: 0.0110\n",
      "iter [180/300], Loss: 0.0103\n",
      "iter [180/300], Loss: 0.0105\n",
      "iter [180/300], Loss: 0.0103\n",
      "iter [180/300], Loss: 0.0110\n",
      "iter [180/300], Loss: 0.0116\n",
      "iter [180/300], Loss: 0.0110\n",
      "iter [180/300], Loss: 0.0108\n",
      "iter [180/300], Loss: 0.0107\n",
      "iter [180/300], Loss: 0.0112\n",
      "iter [180/300], Loss: 0.0111\n",
      "iter [180/300], Loss: 0.0122\n",
      "iter [180/300], Loss: 0.0108\n",
      "iter [180/300], Loss: 0.0108\n",
      "iter [180/300], Loss: 0.0101\n",
      "iter [180/300], Loss: 0.0102\n",
      "iter [180/300], Loss: 0.0108\n",
      "iter [180/300], Loss: 0.0118\n",
      "iter [180/300], Loss: 0.0110\n",
      "iter [180/300], Loss: 0.0125\n",
      "iter [180/300], Loss: 0.0089\n",
      "iter [180/300], Loss: 0.0094\n",
      "iter [180/300], Loss: 0.0095\n",
      "iter [180/300], Loss: 0.0112\n",
      "iter [180/300], Loss: 0.0100\n",
      "iter [180/300], Loss: 0.0110\n",
      "iter [180/300], Loss: 0.0097\n",
      "iter [180/300], Loss: 0.0104\n",
      "iter [180/300], Loss: 0.0095\n",
      "iter [180/300], Loss: 0.0108\n",
      "iter [180/300], Loss: 0.0108\n",
      "iter [180/300], Loss: 0.0117\n",
      "iter [180/300], Loss: 0.0113\n",
      "iter [180/300], Loss: 0.0109\n",
      "iter [180/300], Loss: 0.0105\n",
      "iter [180/300], Loss: 0.0099\n",
      "iter [180/300], Loss: 0.0126\n",
      "iter [180/300], Loss: 0.0105\n",
      "iter [180/300], Loss: 0.0107\n",
      "iter [180/300], Loss: 0.0098\n",
      "iter [180/300], Loss: 0.0134\n",
      "iter [180/300], Loss: 0.0091\n",
      "iter [180/300], Loss: 0.0114\n",
      "iter [180/300], Loss: 0.0109\n",
      "iter [180/300], Loss: 0.0095\n",
      "iter [180/300], Loss: 0.0097\n",
      "iter [180/300], Loss: 0.0091\n",
      "iter [180/300], Loss: 0.0109\n",
      "iter [180/300], Loss: 0.0107\n",
      "iter [180/300], Loss: 0.0103\n",
      "iter [180/300], Loss: 0.0107\n",
      "iter [180/300], Loss: 0.0106\n",
      "iter [180/300], Loss: 0.0097\n",
      "iter [180/300], Loss: 0.0101\n",
      "evaluate loss:0.014349128740529219\n",
      "iter [190/300], Loss: 0.0096\n",
      "iter [190/300], Loss: 0.0115\n",
      "iter [190/300], Loss: 0.0090\n",
      "iter [190/300], Loss: 0.0095\n",
      "iter [190/300], Loss: 0.0113\n",
      "iter [190/300], Loss: 0.0101\n",
      "iter [190/300], Loss: 0.0097\n",
      "iter [190/300], Loss: 0.0094\n",
      "iter [190/300], Loss: 0.0107\n",
      "iter [190/300], Loss: 0.0103\n",
      "iter [190/300], Loss: 0.0084\n",
      "iter [190/300], Loss: 0.0098\n",
      "iter [190/300], Loss: 0.0101\n",
      "iter [190/300], Loss: 0.0107\n",
      "iter [190/300], Loss: 0.0091\n",
      "iter [190/300], Loss: 0.0096\n",
      "iter [190/300], Loss: 0.0106\n",
      "iter [190/300], Loss: 0.0088\n",
      "iter [190/300], Loss: 0.0103\n",
      "iter [190/300], Loss: 0.0095\n",
      "iter [190/300], Loss: 0.0115\n",
      "iter [190/300], Loss: 0.0101\n",
      "iter [190/300], Loss: 0.0098\n",
      "iter [190/300], Loss: 0.0108\n",
      "iter [190/300], Loss: 0.0104\n",
      "iter [190/300], Loss: 0.0101\n",
      "iter [190/300], Loss: 0.0093\n",
      "iter [190/300], Loss: 0.0107\n",
      "iter [190/300], Loss: 0.0092\n",
      "iter [190/300], Loss: 0.0105\n",
      "iter [190/300], Loss: 0.0112\n",
      "iter [190/300], Loss: 0.0097\n",
      "iter [190/300], Loss: 0.0096\n",
      "iter [190/300], Loss: 0.0110\n",
      "iter [190/300], Loss: 0.0108\n",
      "iter [190/300], Loss: 0.0101\n",
      "iter [190/300], Loss: 0.0096\n",
      "iter [190/300], Loss: 0.0115\n",
      "iter [190/300], Loss: 0.0095\n",
      "iter [190/300], Loss: 0.0106\n",
      "iter [190/300], Loss: 0.0113\n",
      "iter [190/300], Loss: 0.0099\n",
      "iter [190/300], Loss: 0.0114\n",
      "iter [190/300], Loss: 0.0101\n",
      "iter [190/300], Loss: 0.0102\n",
      "iter [190/300], Loss: 0.0114\n",
      "iter [190/300], Loss: 0.0100\n",
      "iter [190/300], Loss: 0.0085\n",
      "iter [190/300], Loss: 0.0105\n",
      "iter [190/300], Loss: 0.0092\n",
      "iter [190/300], Loss: 0.0096\n",
      "iter [190/300], Loss: 0.0101\n",
      "iter [190/300], Loss: 0.0102\n",
      "iter [190/300], Loss: 0.0098\n",
      "iter [190/300], Loss: 0.0098\n",
      "iter [190/300], Loss: 0.0106\n",
      "evaluate loss:0.014012637548148632\n",
      "iter [200/300], Loss: 0.0093\n",
      "iter [200/300], Loss: 0.0108\n",
      "iter [200/300], Loss: 0.0098\n",
      "iter [200/300], Loss: 0.0094\n",
      "iter [200/300], Loss: 0.0100\n",
      "iter [200/300], Loss: 0.0095\n",
      "iter [200/300], Loss: 0.0082\n",
      "iter [200/300], Loss: 0.0094\n",
      "iter [200/300], Loss: 0.0094\n",
      "iter [200/300], Loss: 0.0100\n",
      "iter [200/300], Loss: 0.0087\n",
      "iter [200/300], Loss: 0.0096\n",
      "iter [200/300], Loss: 0.0103\n",
      "iter [200/300], Loss: 0.0103\n",
      "iter [200/300], Loss: 0.0109\n",
      "iter [200/300], Loss: 0.0099\n",
      "iter [200/300], Loss: 0.0102\n",
      "iter [200/300], Loss: 0.0106\n",
      "iter [200/300], Loss: 0.0096\n",
      "iter [200/300], Loss: 0.0090\n",
      "iter [200/300], Loss: 0.0102\n",
      "iter [200/300], Loss: 0.0114\n",
      "iter [200/300], Loss: 0.0097\n",
      "iter [200/300], Loss: 0.0098\n",
      "iter [200/300], Loss: 0.0091\n",
      "iter [200/300], Loss: 0.0109\n",
      "iter [200/300], Loss: 0.0091\n",
      "iter [200/300], Loss: 0.0097\n",
      "iter [200/300], Loss: 0.0088\n",
      "iter [200/300], Loss: 0.0086\n",
      "iter [200/300], Loss: 0.0104\n",
      "iter [200/300], Loss: 0.0098\n",
      "iter [200/300], Loss: 0.0074\n",
      "iter [200/300], Loss: 0.0097\n",
      "iter [200/300], Loss: 0.0106\n",
      "iter [200/300], Loss: 0.0096\n",
      "iter [200/300], Loss: 0.0097\n",
      "iter [200/300], Loss: 0.0098\n",
      "iter [200/300], Loss: 0.0100\n",
      "iter [200/300], Loss: 0.0099\n",
      "iter [200/300], Loss: 0.0098\n",
      "iter [200/300], Loss: 0.0101\n",
      "iter [200/300], Loss: 0.0080\n",
      "iter [200/300], Loss: 0.0090\n",
      "iter [200/300], Loss: 0.0103\n",
      "iter [200/300], Loss: 0.0092\n",
      "iter [200/300], Loss: 0.0086\n",
      "iter [200/300], Loss: 0.0090\n",
      "iter [200/300], Loss: 0.0102\n",
      "iter [200/300], Loss: 0.0086\n",
      "iter [200/300], Loss: 0.0087\n",
      "iter [200/300], Loss: 0.0093\n",
      "iter [200/300], Loss: 0.0101\n",
      "iter [200/300], Loss: 0.0106\n",
      "iter [200/300], Loss: 0.0105\n",
      "iter [200/300], Loss: 0.0113\n",
      "evaluate loss:0.013819744810461998\n",
      "iter [210/300], Loss: 0.0082\n",
      "iter [210/300], Loss: 0.0099\n",
      "iter [210/300], Loss: 0.0080\n",
      "iter [210/300], Loss: 0.0086\n",
      "iter [210/300], Loss: 0.0099\n",
      "iter [210/300], Loss: 0.0099\n",
      "iter [210/300], Loss: 0.0088\n",
      "iter [210/300], Loss: 0.0092\n",
      "iter [210/300], Loss: 0.0100\n",
      "iter [210/300], Loss: 0.0083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter [210/300], Loss: 0.0090\n",
      "iter [210/300], Loss: 0.0083\n",
      "iter [210/300], Loss: 0.0095\n",
      "iter [210/300], Loss: 0.0090\n",
      "iter [210/300], Loss: 0.0098\n",
      "iter [210/300], Loss: 0.0102\n",
      "iter [210/300], Loss: 0.0109\n",
      "iter [210/300], Loss: 0.0105\n",
      "iter [210/300], Loss: 0.0093\n",
      "iter [210/300], Loss: 0.0094\n",
      "iter [210/300], Loss: 0.0077\n",
      "iter [210/300], Loss: 0.0088\n",
      "iter [210/300], Loss: 0.0093\n",
      "iter [210/300], Loss: 0.0106\n",
      "iter [210/300], Loss: 0.0086\n",
      "iter [210/300], Loss: 0.0091\n",
      "iter [210/300], Loss: 0.0091\n",
      "iter [210/300], Loss: 0.0083\n",
      "iter [210/300], Loss: 0.0093\n",
      "iter [210/300], Loss: 0.0108\n",
      "iter [210/300], Loss: 0.0099\n",
      "iter [210/300], Loss: 0.0083\n",
      "iter [210/300], Loss: 0.0090\n",
      "iter [210/300], Loss: 0.0088\n",
      "iter [210/300], Loss: 0.0087\n",
      "iter [210/300], Loss: 0.0083\n",
      "iter [210/300], Loss: 0.0082\n",
      "iter [210/300], Loss: 0.0094\n",
      "iter [210/300], Loss: 0.0097\n",
      "iter [210/300], Loss: 0.0094\n",
      "iter [210/300], Loss: 0.0086\n",
      "iter [210/300], Loss: 0.0084\n",
      "iter [210/300], Loss: 0.0091\n",
      "iter [210/300], Loss: 0.0093\n",
      "iter [210/300], Loss: 0.0083\n",
      "iter [210/300], Loss: 0.0095\n",
      "iter [210/300], Loss: 0.0097\n",
      "iter [210/300], Loss: 0.0090\n",
      "iter [210/300], Loss: 0.0104\n",
      "iter [210/300], Loss: 0.0114\n",
      "iter [210/300], Loss: 0.0103\n",
      "iter [210/300], Loss: 0.0093\n",
      "iter [210/300], Loss: 0.0092\n",
      "iter [210/300], Loss: 0.0100\n",
      "iter [210/300], Loss: 0.0085\n",
      "iter [210/300], Loss: 0.0096\n",
      "evaluate loss:0.013522653840482235\n",
      "iter [220/300], Loss: 0.0091\n",
      "iter [220/300], Loss: 0.0093\n",
      "iter [220/300], Loss: 0.0088\n",
      "iter [220/300], Loss: 0.0084\n",
      "iter [220/300], Loss: 0.0094\n",
      "iter [220/300], Loss: 0.0084\n",
      "iter [220/300], Loss: 0.0086\n",
      "iter [220/300], Loss: 0.0097\n",
      "iter [220/300], Loss: 0.0089\n",
      "iter [220/300], Loss: 0.0087\n",
      "iter [220/300], Loss: 0.0084\n",
      "iter [220/300], Loss: 0.0097\n",
      "iter [220/300], Loss: 0.0085\n",
      "iter [220/300], Loss: 0.0084\n",
      "iter [220/300], Loss: 0.0083\n",
      "iter [220/300], Loss: 0.0092\n",
      "iter [220/300], Loss: 0.0102\n",
      "iter [220/300], Loss: 0.0086\n",
      "iter [220/300], Loss: 0.0090\n",
      "iter [220/300], Loss: 0.0096\n",
      "iter [220/300], Loss: 0.0085\n",
      "iter [220/300], Loss: 0.0096\n",
      "iter [220/300], Loss: 0.0083\n",
      "iter [220/300], Loss: 0.0083\n",
      "iter [220/300], Loss: 0.0079\n",
      "iter [220/300], Loss: 0.0090\n",
      "iter [220/300], Loss: 0.0092\n",
      "iter [220/300], Loss: 0.0089\n",
      "iter [220/300], Loss: 0.0086\n",
      "iter [220/300], Loss: 0.0101\n",
      "iter [220/300], Loss: 0.0086\n",
      "iter [220/300], Loss: 0.0084\n",
      "iter [220/300], Loss: 0.0091\n",
      "iter [220/300], Loss: 0.0090\n",
      "iter [220/300], Loss: 0.0093\n",
      "iter [220/300], Loss: 0.0086\n",
      "iter [220/300], Loss: 0.0090\n",
      "iter [220/300], Loss: 0.0082\n",
      "iter [220/300], Loss: 0.0093\n",
      "iter [220/300], Loss: 0.0086\n",
      "iter [220/300], Loss: 0.0108\n",
      "iter [220/300], Loss: 0.0080\n",
      "iter [220/300], Loss: 0.0090\n",
      "iter [220/300], Loss: 0.0094\n",
      "iter [220/300], Loss: 0.0091\n",
      "iter [220/300], Loss: 0.0072\n",
      "iter [220/300], Loss: 0.0076\n",
      "iter [220/300], Loss: 0.0087\n",
      "iter [220/300], Loss: 0.0092\n",
      "iter [220/300], Loss: 0.0086\n",
      "iter [220/300], Loss: 0.0082\n",
      "iter [220/300], Loss: 0.0092\n",
      "iter [220/300], Loss: 0.0090\n",
      "iter [220/300], Loss: 0.0083\n",
      "iter [220/300], Loss: 0.0087\n",
      "iter [220/300], Loss: 0.0095\n",
      "evaluate loss:0.0132840850080053\n",
      "iter [230/300], Loss: 0.0097\n",
      "iter [230/300], Loss: 0.0084\n",
      "iter [230/300], Loss: 0.0084\n",
      "iter [230/300], Loss: 0.0088\n",
      "iter [230/300], Loss: 0.0101\n",
      "iter [230/300], Loss: 0.0077\n",
      "iter [230/300], Loss: 0.0073\n",
      "iter [230/300], Loss: 0.0098\n",
      "iter [230/300], Loss: 0.0075\n",
      "iter [230/300], Loss: 0.0094\n",
      "iter [230/300], Loss: 0.0095\n",
      "iter [230/300], Loss: 0.0088\n",
      "iter [230/300], Loss: 0.0089\n",
      "iter [230/300], Loss: 0.0098\n",
      "iter [230/300], Loss: 0.0099\n",
      "iter [230/300], Loss: 0.0074\n",
      "iter [230/300], Loss: 0.0077\n",
      "iter [230/300], Loss: 0.0090\n",
      "iter [230/300], Loss: 0.0066\n",
      "iter [230/300], Loss: 0.0084\n",
      "iter [230/300], Loss: 0.0082\n",
      "iter [230/300], Loss: 0.0089\n",
      "iter [230/300], Loss: 0.0074\n",
      "iter [230/300], Loss: 0.0088\n",
      "iter [230/300], Loss: 0.0089\n",
      "iter [230/300], Loss: 0.0073\n",
      "iter [230/300], Loss: 0.0087\n",
      "iter [230/300], Loss: 0.0078\n",
      "iter [230/300], Loss: 0.0084\n",
      "iter [230/300], Loss: 0.0086\n",
      "iter [230/300], Loss: 0.0084\n",
      "iter [230/300], Loss: 0.0086\n",
      "iter [230/300], Loss: 0.0078\n",
      "iter [230/300], Loss: 0.0087\n",
      "iter [230/300], Loss: 0.0081\n",
      "iter [230/300], Loss: 0.0079\n",
      "iter [230/300], Loss: 0.0076\n",
      "iter [230/300], Loss: 0.0078\n",
      "iter [230/300], Loss: 0.0090\n",
      "iter [230/300], Loss: 0.0091\n",
      "iter [230/300], Loss: 0.0097\n",
      "iter [230/300], Loss: 0.0079\n",
      "iter [230/300], Loss: 0.0084\n",
      "iter [230/300], Loss: 0.0092\n",
      "iter [230/300], Loss: 0.0102\n",
      "iter [230/300], Loss: 0.0089\n",
      "iter [230/300], Loss: 0.0087\n",
      "iter [230/300], Loss: 0.0086\n",
      "iter [230/300], Loss: 0.0079\n",
      "iter [230/300], Loss: 0.0095\n",
      "iter [230/300], Loss: 0.0086\n",
      "iter [230/300], Loss: 0.0091\n",
      "iter [230/300], Loss: 0.0072\n",
      "iter [230/300], Loss: 0.0068\n",
      "iter [230/300], Loss: 0.0090\n",
      "iter [230/300], Loss: 0.0068\n",
      "evaluate loss:0.013098196436961492\n",
      "iter [240/300], Loss: 0.0079\n",
      "iter [240/300], Loss: 0.0071\n",
      "iter [240/300], Loss: 0.0074\n",
      "iter [240/300], Loss: 0.0087\n",
      "iter [240/300], Loss: 0.0084\n",
      "iter [240/300], Loss: 0.0086\n",
      "iter [240/300], Loss: 0.0083\n",
      "iter [240/300], Loss: 0.0076\n",
      "iter [240/300], Loss: 0.0077\n",
      "iter [240/300], Loss: 0.0074\n",
      "iter [240/300], Loss: 0.0089\n",
      "iter [240/300], Loss: 0.0086\n",
      "iter [240/300], Loss: 0.0071\n",
      "iter [240/300], Loss: 0.0078\n",
      "iter [240/300], Loss: 0.0068\n",
      "iter [240/300], Loss: 0.0078\n",
      "iter [240/300], Loss: 0.0092\n",
      "iter [240/300], Loss: 0.0085\n",
      "iter [240/300], Loss: 0.0084\n",
      "iter [240/300], Loss: 0.0088\n",
      "iter [240/300], Loss: 0.0094\n",
      "iter [240/300], Loss: 0.0071\n",
      "iter [240/300], Loss: 0.0075\n",
      "iter [240/300], Loss: 0.0078\n",
      "iter [240/300], Loss: 0.0088\n",
      "iter [240/300], Loss: 0.0084\n",
      "iter [240/300], Loss: 0.0082\n",
      "iter [240/300], Loss: 0.0081\n",
      "iter [240/300], Loss: 0.0093\n",
      "iter [240/300], Loss: 0.0081\n",
      "iter [240/300], Loss: 0.0069\n",
      "iter [240/300], Loss: 0.0080\n",
      "iter [240/300], Loss: 0.0085\n",
      "iter [240/300], Loss: 0.0085\n",
      "iter [240/300], Loss: 0.0095\n",
      "iter [240/300], Loss: 0.0100\n",
      "iter [240/300], Loss: 0.0070\n",
      "iter [240/300], Loss: 0.0102\n",
      "iter [240/300], Loss: 0.0086\n",
      "iter [240/300], Loss: 0.0084\n",
      "iter [240/300], Loss: 0.0077\n",
      "iter [240/300], Loss: 0.0078\n",
      "iter [240/300], Loss: 0.0083\n",
      "iter [240/300], Loss: 0.0076\n",
      "iter [240/300], Loss: 0.0073\n",
      "iter [240/300], Loss: 0.0077\n",
      "iter [240/300], Loss: 0.0077\n",
      "iter [240/300], Loss: 0.0082\n",
      "iter [240/300], Loss: 0.0088\n",
      "iter [240/300], Loss: 0.0076\n",
      "iter [240/300], Loss: 0.0081\n",
      "iter [240/300], Loss: 0.0080\n",
      "iter [240/300], Loss: 0.0084\n",
      "iter [240/300], Loss: 0.0087\n",
      "iter [240/300], Loss: 0.0082\n",
      "iter [240/300], Loss: 0.0089\n",
      "evaluate loss:0.012913579742113749\n",
      "iter [250/300], Loss: 0.0083\n",
      "iter [250/300], Loss: 0.0075\n",
      "iter [250/300], Loss: 0.0079\n",
      "iter [250/300], Loss: 0.0084\n",
      "iter [250/300], Loss: 0.0069\n",
      "iter [250/300], Loss: 0.0077\n",
      "iter [250/300], Loss: 0.0082\n",
      "iter [250/300], Loss: 0.0070\n",
      "iter [250/300], Loss: 0.0074\n",
      "iter [250/300], Loss: 0.0083\n",
      "iter [250/300], Loss: 0.0065\n",
      "iter [250/300], Loss: 0.0091\n",
      "iter [250/300], Loss: 0.0079\n",
      "iter [250/300], Loss: 0.0074\n",
      "iter [250/300], Loss: 0.0081\n",
      "iter [250/300], Loss: 0.0079\n",
      "iter [250/300], Loss: 0.0084\n",
      "iter [250/300], Loss: 0.0083\n",
      "iter [250/300], Loss: 0.0075\n",
      "iter [250/300], Loss: 0.0080\n",
      "iter [250/300], Loss: 0.0072\n",
      "iter [250/300], Loss: 0.0081\n",
      "iter [250/300], Loss: 0.0089\n",
      "iter [250/300], Loss: 0.0077\n",
      "iter [250/300], Loss: 0.0066\n",
      "iter [250/300], Loss: 0.0076\n",
      "iter [250/300], Loss: 0.0070\n",
      "iter [250/300], Loss: 0.0073\n",
      "iter [250/300], Loss: 0.0080\n",
      "iter [250/300], Loss: 0.0084\n",
      "iter [250/300], Loss: 0.0082\n",
      "iter [250/300], Loss: 0.0079\n",
      "iter [250/300], Loss: 0.0076\n",
      "iter [250/300], Loss: 0.0079\n",
      "iter [250/300], Loss: 0.0085\n",
      "iter [250/300], Loss: 0.0078\n",
      "iter [250/300], Loss: 0.0081\n",
      "iter [250/300], Loss: 0.0074\n",
      "iter [250/300], Loss: 0.0075\n",
      "iter [250/300], Loss: 0.0077\n",
      "iter [250/300], Loss: 0.0078\n",
      "iter [250/300], Loss: 0.0071\n",
      "iter [250/300], Loss: 0.0082\n",
      "iter [250/300], Loss: 0.0086\n",
      "iter [250/300], Loss: 0.0080\n",
      "iter [250/300], Loss: 0.0088\n",
      "iter [250/300], Loss: 0.0079\n",
      "iter [250/300], Loss: 0.0070\n",
      "iter [250/300], Loss: 0.0088\n",
      "iter [250/300], Loss: 0.0067\n",
      "iter [250/300], Loss: 0.0085\n",
      "iter [250/300], Loss: 0.0068\n",
      "iter [250/300], Loss: 0.0078\n",
      "iter [250/300], Loss: 0.0081\n",
      "iter [250/300], Loss: 0.0075\n",
      "iter [250/300], Loss: 0.0085\n",
      "evaluate loss:0.012776038299004236\n",
      "iter [260/300], Loss: 0.0067\n",
      "iter [260/300], Loss: 0.0078\n",
      "iter [260/300], Loss: 0.0072\n",
      "iter [260/300], Loss: 0.0077\n",
      "iter [260/300], Loss: 0.0064\n",
      "iter [260/300], Loss: 0.0098\n",
      "iter [260/300], Loss: 0.0067\n",
      "iter [260/300], Loss: 0.0076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter [260/300], Loss: 0.0070\n",
      "iter [260/300], Loss: 0.0074\n",
      "iter [260/300], Loss: 0.0066\n",
      "iter [260/300], Loss: 0.0075\n",
      "iter [260/300], Loss: 0.0074\n",
      "iter [260/300], Loss: 0.0082\n",
      "iter [260/300], Loss: 0.0078\n",
      "iter [260/300], Loss: 0.0087\n",
      "iter [260/300], Loss: 0.0080\n",
      "iter [260/300], Loss: 0.0076\n",
      "iter [260/300], Loss: 0.0089\n",
      "iter [260/300], Loss: 0.0077\n",
      "iter [260/300], Loss: 0.0072\n",
      "iter [260/300], Loss: 0.0081\n",
      "iter [260/300], Loss: 0.0074\n",
      "iter [260/300], Loss: 0.0082\n",
      "iter [260/300], Loss: 0.0076\n",
      "iter [260/300], Loss: 0.0077\n",
      "iter [260/300], Loss: 0.0072\n",
      "iter [260/300], Loss: 0.0078\n",
      "iter [260/300], Loss: 0.0072\n",
      "iter [260/300], Loss: 0.0072\n",
      "iter [260/300], Loss: 0.0072\n",
      "iter [260/300], Loss: 0.0075\n",
      "iter [260/300], Loss: 0.0077\n",
      "iter [260/300], Loss: 0.0062\n",
      "iter [260/300], Loss: 0.0069\n",
      "iter [260/300], Loss: 0.0078\n",
      "iter [260/300], Loss: 0.0072\n",
      "iter [260/300], Loss: 0.0071\n",
      "iter [260/300], Loss: 0.0085\n",
      "iter [260/300], Loss: 0.0086\n",
      "iter [260/300], Loss: 0.0076\n",
      "iter [260/300], Loss: 0.0068\n",
      "iter [260/300], Loss: 0.0083\n",
      "iter [260/300], Loss: 0.0078\n",
      "iter [260/300], Loss: 0.0070\n",
      "iter [260/300], Loss: 0.0061\n",
      "iter [260/300], Loss: 0.0070\n",
      "iter [260/300], Loss: 0.0076\n",
      "iter [260/300], Loss: 0.0073\n",
      "iter [260/300], Loss: 0.0070\n",
      "iter [260/300], Loss: 0.0074\n",
      "iter [260/300], Loss: 0.0085\n",
      "iter [260/300], Loss: 0.0072\n",
      "iter [260/300], Loss: 0.0068\n",
      "iter [260/300], Loss: 0.0084\n",
      "iter [260/300], Loss: 0.0080\n",
      "evaluate loss:0.012538591710229715\n",
      "iter [270/300], Loss: 0.0067\n",
      "iter [270/300], Loss: 0.0082\n",
      "iter [270/300], Loss: 0.0069\n",
      "iter [270/300], Loss: 0.0074\n",
      "iter [270/300], Loss: 0.0067\n",
      "iter [270/300], Loss: 0.0063\n",
      "iter [270/300], Loss: 0.0068\n",
      "iter [270/300], Loss: 0.0073\n",
      "iter [270/300], Loss: 0.0075\n",
      "iter [270/300], Loss: 0.0079\n",
      "iter [270/300], Loss: 0.0082\n",
      "iter [270/300], Loss: 0.0068\n",
      "iter [270/300], Loss: 0.0069\n",
      "iter [270/300], Loss: 0.0071\n",
      "iter [270/300], Loss: 0.0071\n",
      "iter [270/300], Loss: 0.0067\n",
      "iter [270/300], Loss: 0.0072\n",
      "iter [270/300], Loss: 0.0083\n",
      "iter [270/300], Loss: 0.0075\n",
      "iter [270/300], Loss: 0.0086\n",
      "iter [270/300], Loss: 0.0059\n",
      "iter [270/300], Loss: 0.0075\n",
      "iter [270/300], Loss: 0.0079\n",
      "iter [270/300], Loss: 0.0071\n",
      "iter [270/300], Loss: 0.0077\n",
      "iter [270/300], Loss: 0.0081\n",
      "iter [270/300], Loss: 0.0068\n",
      "iter [270/300], Loss: 0.0078\n",
      "iter [270/300], Loss: 0.0081\n",
      "iter [270/300], Loss: 0.0060\n",
      "iter [270/300], Loss: 0.0073\n",
      "iter [270/300], Loss: 0.0071\n",
      "iter [270/300], Loss: 0.0059\n",
      "iter [270/300], Loss: 0.0067\n",
      "iter [270/300], Loss: 0.0059\n",
      "iter [270/300], Loss: 0.0066\n",
      "iter [270/300], Loss: 0.0071\n",
      "iter [270/300], Loss: 0.0075\n",
      "iter [270/300], Loss: 0.0068\n",
      "iter [270/300], Loss: 0.0070\n",
      "iter [270/300], Loss: 0.0074\n",
      "iter [270/300], Loss: 0.0069\n",
      "iter [270/300], Loss: 0.0070\n",
      "iter [270/300], Loss: 0.0086\n",
      "iter [270/300], Loss: 0.0086\n",
      "iter [270/300], Loss: 0.0083\n",
      "iter [270/300], Loss: 0.0080\n",
      "iter [270/300], Loss: 0.0079\n",
      "iter [270/300], Loss: 0.0060\n",
      "iter [270/300], Loss: 0.0083\n",
      "iter [270/300], Loss: 0.0060\n",
      "iter [270/300], Loss: 0.0077\n",
      "iter [270/300], Loss: 0.0075\n",
      "iter [270/300], Loss: 0.0075\n",
      "iter [270/300], Loss: 0.0066\n",
      "iter [270/300], Loss: 0.0071\n",
      "evaluate loss:0.012447185503939787\n",
      "iter [280/300], Loss: 0.0073\n",
      "iter [280/300], Loss: 0.0061\n",
      "iter [280/300], Loss: 0.0073\n",
      "iter [280/300], Loss: 0.0078\n",
      "iter [280/300], Loss: 0.0068\n",
      "iter [280/300], Loss: 0.0069\n",
      "iter [280/300], Loss: 0.0089\n",
      "iter [280/300], Loss: 0.0070\n",
      "iter [280/300], Loss: 0.0060\n",
      "iter [280/300], Loss: 0.0072\n",
      "iter [280/300], Loss: 0.0067\n",
      "iter [280/300], Loss: 0.0069\n",
      "iter [280/300], Loss: 0.0074\n",
      "iter [280/300], Loss: 0.0063\n",
      "iter [280/300], Loss: 0.0072\n",
      "iter [280/300], Loss: 0.0074\n",
      "iter [280/300], Loss: 0.0075\n",
      "iter [280/300], Loss: 0.0064\n",
      "iter [280/300], Loss: 0.0066\n",
      "iter [280/300], Loss: 0.0081\n",
      "iter [280/300], Loss: 0.0066\n",
      "iter [280/300], Loss: 0.0065\n",
      "iter [280/300], Loss: 0.0071\n",
      "iter [280/300], Loss: 0.0062\n",
      "iter [280/300], Loss: 0.0064\n",
      "iter [280/300], Loss: 0.0072\n",
      "iter [280/300], Loss: 0.0066\n",
      "iter [280/300], Loss: 0.0071\n",
      "iter [280/300], Loss: 0.0078\n",
      "iter [280/300], Loss: 0.0066\n",
      "iter [280/300], Loss: 0.0062\n",
      "iter [280/300], Loss: 0.0065\n",
      "iter [280/300], Loss: 0.0064\n",
      "iter [280/300], Loss: 0.0067\n",
      "iter [280/300], Loss: 0.0067\n",
      "iter [280/300], Loss: 0.0072\n",
      "iter [280/300], Loss: 0.0072\n",
      "iter [280/300], Loss: 0.0065\n",
      "iter [280/300], Loss: 0.0065\n",
      "iter [280/300], Loss: 0.0079\n",
      "iter [280/300], Loss: 0.0070\n",
      "iter [280/300], Loss: 0.0073\n",
      "iter [280/300], Loss: 0.0067\n",
      "iter [280/300], Loss: 0.0067\n",
      "iter [280/300], Loss: 0.0075\n",
      "iter [280/300], Loss: 0.0070\n",
      "iter [280/300], Loss: 0.0062\n",
      "iter [280/300], Loss: 0.0066\n",
      "iter [280/300], Loss: 0.0069\n",
      "iter [280/300], Loss: 0.0083\n",
      "iter [280/300], Loss: 0.0069\n",
      "iter [280/300], Loss: 0.0070\n",
      "iter [280/300], Loss: 0.0070\n",
      "iter [280/300], Loss: 0.0077\n",
      "iter [280/300], Loss: 0.0076\n",
      "iter [280/300], Loss: 0.0077\n",
      "evaluate loss:0.012313831908007463\n",
      "iter [290/300], Loss: 0.0066\n",
      "iter [290/300], Loss: 0.0065\n",
      "iter [290/300], Loss: 0.0076\n",
      "iter [290/300], Loss: 0.0070\n",
      "iter [290/300], Loss: 0.0060\n",
      "iter [290/300], Loss: 0.0060\n",
      "iter [290/300], Loss: 0.0065\n",
      "iter [290/300], Loss: 0.0067\n",
      "iter [290/300], Loss: 0.0064\n",
      "iter [290/300], Loss: 0.0068\n",
      "iter [290/300], Loss: 0.0066\n",
      "iter [290/300], Loss: 0.0068\n",
      "iter [290/300], Loss: 0.0062\n",
      "iter [290/300], Loss: 0.0059\n",
      "iter [290/300], Loss: 0.0069\n",
      "iter [290/300], Loss: 0.0069\n",
      "iter [290/300], Loss: 0.0064\n",
      "iter [290/300], Loss: 0.0071\n",
      "iter [290/300], Loss: 0.0059\n",
      "iter [290/300], Loss: 0.0071\n",
      "iter [290/300], Loss: 0.0064\n",
      "iter [290/300], Loss: 0.0070\n",
      "iter [290/300], Loss: 0.0065\n",
      "iter [290/300], Loss: 0.0067\n",
      "iter [290/300], Loss: 0.0068\n",
      "iter [290/300], Loss: 0.0066\n",
      "iter [290/300], Loss: 0.0063\n",
      "iter [290/300], Loss: 0.0078\n",
      "iter [290/300], Loss: 0.0064\n",
      "iter [290/300], Loss: 0.0069\n",
      "iter [290/300], Loss: 0.0068\n",
      "iter [290/300], Loss: 0.0072\n",
      "iter [290/300], Loss: 0.0064\n",
      "iter [290/300], Loss: 0.0076\n",
      "iter [290/300], Loss: 0.0074\n",
      "iter [290/300], Loss: 0.0065\n",
      "iter [290/300], Loss: 0.0071\n",
      "iter [290/300], Loss: 0.0069\n",
      "iter [290/300], Loss: 0.0063\n",
      "iter [290/300], Loss: 0.0072\n",
      "iter [290/300], Loss: 0.0063\n",
      "iter [290/300], Loss: 0.0074\n",
      "iter [290/300], Loss: 0.0066\n",
      "iter [290/300], Loss: 0.0074\n",
      "iter [290/300], Loss: 0.0062\n",
      "iter [290/300], Loss: 0.0067\n",
      "iter [290/300], Loss: 0.0069\n",
      "iter [290/300], Loss: 0.0063\n",
      "iter [290/300], Loss: 0.0066\n",
      "iter [290/300], Loss: 0.0064\n",
      "iter [290/300], Loss: 0.0069\n",
      "iter [290/300], Loss: 0.0071\n",
      "iter [290/300], Loss: 0.0072\n",
      "iter [290/300], Loss: 0.0076\n",
      "iter [290/300], Loss: 0.0055\n",
      "iter [290/300], Loss: 0.0079\n",
      "evaluate loss:0.012224592578907808\n",
      "iter [300/300], Loss: 0.0062\n",
      "iter [300/300], Loss: 0.0062\n",
      "iter [300/300], Loss: 0.0073\n",
      "iter [300/300], Loss: 0.0065\n",
      "iter [300/300], Loss: 0.0067\n",
      "iter [300/300], Loss: 0.0068\n",
      "iter [300/300], Loss: 0.0062\n",
      "iter [300/300], Loss: 0.0064\n",
      "iter [300/300], Loss: 0.0074\n",
      "iter [300/300], Loss: 0.0067\n",
      "iter [300/300], Loss: 0.0057\n",
      "iter [300/300], Loss: 0.0072\n",
      "iter [300/300], Loss: 0.0061\n",
      "iter [300/300], Loss: 0.0070\n",
      "iter [300/300], Loss: 0.0065\n",
      "iter [300/300], Loss: 0.0072\n",
      "iter [300/300], Loss: 0.0069\n",
      "iter [300/300], Loss: 0.0050\n",
      "iter [300/300], Loss: 0.0072\n",
      "iter [300/300], Loss: 0.0059\n",
      "iter [300/300], Loss: 0.0061\n",
      "iter [300/300], Loss: 0.0070\n",
      "iter [300/300], Loss: 0.0069\n",
      "iter [300/300], Loss: 0.0070\n",
      "iter [300/300], Loss: 0.0069\n",
      "iter [300/300], Loss: 0.0068\n",
      "iter [300/300], Loss: 0.0057\n",
      "iter [300/300], Loss: 0.0058\n",
      "iter [300/300], Loss: 0.0065\n",
      "iter [300/300], Loss: 0.0067\n",
      "iter [300/300], Loss: 0.0073\n",
      "iter [300/300], Loss: 0.0065\n",
      "iter [300/300], Loss: 0.0055\n",
      "iter [300/300], Loss: 0.0066\n",
      "iter [300/300], Loss: 0.0066\n",
      "iter [300/300], Loss: 0.0063\n",
      "iter [300/300], Loss: 0.0069\n",
      "iter [300/300], Loss: 0.0057\n",
      "iter [300/300], Loss: 0.0057\n",
      "iter [300/300], Loss: 0.0068\n",
      "iter [300/300], Loss: 0.0063\n",
      "iter [300/300], Loss: 0.0067\n",
      "iter [300/300], Loss: 0.0071\n",
      "iter [300/300], Loss: 0.0064\n",
      "iter [300/300], Loss: 0.0064\n",
      "iter [300/300], Loss: 0.0062\n",
      "iter [300/300], Loss: 0.0062\n",
      "iter [300/300], Loss: 0.0054\n",
      "iter [300/300], Loss: 0.0061\n",
      "iter [300/300], Loss: 0.0065\n",
      "iter [300/300], Loss: 0.0069\n",
      "iter [300/300], Loss: 0.0061\n",
      "iter [300/300], Loss: 0.0069\n",
      "iter [300/300], Loss: 0.0065\n",
      "iter [300/300], Loss: 0.0058\n",
      "iter [300/300], Loss: 0.0074\n",
      "evaluate loss:0.012094842580457529\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(os.getcwd()+'/log', comment='textcnn')\n",
    "\n",
    "# 训练\n",
    "\n",
    "# 构建model\n",
    "model = TextCNN(len(TEXT.vocab),TEXT.vocab.vectors.shape[1],73).to(DEVICE)\n",
    "#初始化权重\n",
    "model.apply(init_weights)\n",
    "# 利用预训练模型初始化embedding，requires_grad=True，可以fine-tune\n",
    "model.embedding.weight.data.copy_(TEXT.vocab.vectors)\n",
    "# 训练模式\n",
    "model.train()\n",
    "# 优化和损失\n",
    "#optimizer = torch.optim.Adam(model.parameters(),lr=0.1, weight_decay=0.01)\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.1, momentum=0.9, nesterov=True)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "with writer:\n",
    "    for iter in range(300):\n",
    "        for i, batch in enumerate(train_iterator):\n",
    "            train_text = batch.text\n",
    "            train_label = batch.label\n",
    "            train_label = train_label.float()\n",
    "            \n",
    "            train_text = train_text.to(DEVICE)\n",
    "            train_label = train_label.to(DEVICE)\n",
    "            out = model(train_text)\n",
    "            loss = criterion(out, train_label)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if (iter+1) % 10 == 0:\n",
    "                    print ('iter [{}/{}], Loss: {:.4f}'.format(iter+1, 300, loss.item()))\n",
    "            #writer.add_graph(model, input_to_model=train_text,verbose=False)\n",
    "            writer.add_scalar('loss',loss.item(),global_step=iter+1)\n",
    "        if (iter+1) % 10 == 0:\n",
    "            evaluate(model, criterion)\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "            \n",
    "model_path = os.path.join(os.getcwd(), \"model.h5\")\n",
    "torch.save(model.state_dict(), model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![textcnn模型](img/loss.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 释放gpu显存\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
